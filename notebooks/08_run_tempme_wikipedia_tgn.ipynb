{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run TempME for Wikipedia + TGN\n",
    "\n",
    "This notebook runs the TempME submodule (`learn_base.py` + `temp_exp_main.py`) for `data=wikipedia` and `base_type=tgn`, then parses and saves run metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain\n",
      "TEMP_ME_ROOT: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/explainer/TempME\n",
      "PYTHON_BIN: /Users/juliawenkmann/miniconda3/envs/graphs/bin/python\n",
      "QUICK_RUN: True\n",
      "BASE_OVERRIDES: {'gpu': -1, 'n_epoch': 1, 'bs': 1024}\n",
      "EXPLAINER_OVERRIDES: {'gpu': -1, 'n_epoch': 1, 'bs': 512, 'test_bs': 512, 'verbose': 1}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _bootstrap_repo_root(start: Path | None = None) -> Path:\n",
    "    here = (start or Path.cwd()).resolve()\n",
    "    for candidate in (here, *here.parents):\n",
    "        if (candidate / \"time_to_explain\").is_dir() and (candidate / \"notebooks\").is_dir():\n",
    "            return candidate\n",
    "    raise RuntimeError(f\"Could not locate repository root from {here}\")\n",
    "\n",
    "\n",
    "PROJECT_ROOT = _bootstrap_repo_root()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "TEMP_ME_ROOT_CANDIDATES = [\n",
    "    PROJECT_ROOT / \"submodules\" / \"explainer\" / \"TempME\",\n",
    "    PROJECT_ROOT / \"submodules\" / \"explainer\" / \"tempme\",\n",
    "]\n",
    "TEMP_ME_ROOT = next((p for p in TEMP_ME_ROOT_CANDIDATES if p.exists()), None)\n",
    "if TEMP_ME_ROOT is None:\n",
    "    raise FileNotFoundError(\"Could not find TempME submodule under submodules/explainer\")\n",
    "\n",
    "DATASET_NAME = \"wikipedia\"\n",
    "BASE_TYPE = \"tgn\"\n",
    "\n",
    "# Quick mode is practical on CPU. Set False for full official epochs on GPU.\n",
    "QUICK_RUN = True\n",
    "FORCE_RERUN_BASE = False\n",
    "FORCE_RERUN_EXPLAINER = False\n",
    "\n",
    "graphs_python = Path(\"/Users/juliawenkmann/miniconda3/envs/graphs/bin/python\")\n",
    "PYTHON_BIN = str(graphs_python if graphs_python.exists() else Path(sys.executable))\n",
    "\n",
    "if QUICK_RUN:\n",
    "    BASE_OVERRIDES = {\"gpu\": -1, \"n_epoch\": 1, \"bs\": 1024}\n",
    "    EXPLAINER_OVERRIDES = {\"gpu\": -1, \"n_epoch\": 1, \"bs\": 512, \"test_bs\": 512, \"verbose\": 1}\n",
    "else:\n",
    "    BASE_OVERRIDES = {\"gpu\": 0}\n",
    "    EXPLAINER_OVERRIDES = {\"gpu\": 0, \"verbose\": 1}\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"TEMP_ME_ROOT:\", TEMP_ME_ROOT)\n",
    "print(\"PYTHON_BIN:\", PYTHON_BIN)\n",
    "print(\"QUICK_RUN:\", QUICK_RUN)\n",
    "print(\"BASE_OVERRIDES:\", BASE_OVERRIDES)\n",
    "print(\"EXPLAINER_OVERRIDES:\", EXPLAINER_OVERRIDES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TempME preprocessed packs already present.\n",
      "Base checkpoint: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/explainer/TempME/params/tgnn/tgn_wikipedia.pt\n",
      "Explainer checkpoint: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/explainer/TempME/params/explainer/tgn/wikipedia.pt\n"
     ]
    }
   ],
   "source": [
    "from shutil import copy2\n",
    "\n",
    "RESOURCES_PROCESSED = PROJECT_ROOT / \"resources\" / \"datasets\" / \"processed\"\n",
    "TEMP_ME_PROCESSED = TEMP_ME_ROOT / \"processed\"\n",
    "TEMP_ME_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for name in (\n",
    "    f\"ml_{DATASET_NAME}.csv\",\n",
    "    f\"ml_{DATASET_NAME}.npy\",\n",
    "    f\"ml_{DATASET_NAME}_node.npy\",\n",
    "):\n",
    "    src = RESOURCES_PROCESSED / name\n",
    "    dst = TEMP_ME_PROCESSED / name\n",
    "    if src.exists() and not dst.exists():\n",
    "        copy2(src, dst)\n",
    "        print(f\"Copied {src} -> {dst}\")\n",
    "\n",
    "required_pack = [\n",
    "    TEMP_ME_PROCESSED / f\"{DATASET_NAME}_train_cat.h5\",\n",
    "    TEMP_ME_PROCESSED / f\"{DATASET_NAME}_test_cat.h5\",\n",
    "    TEMP_ME_PROCESSED / f\"{DATASET_NAME}_train_edge.npy\",\n",
    "    TEMP_ME_PROCESSED / f\"{DATASET_NAME}_test_edge.npy\",\n",
    "]\n",
    "if not all(p.exists() for p in required_pack):\n",
    "    from time_to_explain.data.tempme_preprocess import TempMEPreprocessConfig, prepare_tempme_dataset\n",
    "\n",
    "    cfg = TempMEPreprocessConfig(\n",
    "        dataset_name=DATASET_NAME,\n",
    "        processed_dir=RESOURCES_PROCESSED,\n",
    "        output_dir=TEMP_ME_PROCESSED,\n",
    "        overwrite=False,\n",
    "        validate_existing=False,\n",
    "    )\n",
    "    out = prepare_tempme_dataset(cfg)\n",
    "    print(\"Prepared TempME packs:\", out)\n",
    "else:\n",
    "    print(\"TempME preprocessed packs already present.\")\n",
    "\n",
    "PARAMS_ROOT = TEMP_ME_ROOT / \"params\"\n",
    "BASE_CKPT = PARAMS_ROOT / \"tgnn\" / f\"{BASE_TYPE}_{DATASET_NAME}.pt\"\n",
    "EXPL_CKPT = PARAMS_ROOT / \"explainer\" / BASE_TYPE / f\"{DATASET_NAME}.pt\"\n",
    "\n",
    "print(\"Base checkpoint:\", BASE_CKPT)\n",
    "print(\"Explainer checkpoint:\", EXPL_CKPT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import shlex\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _cli_args(args: dict[str, object]) -> list[str]:\n",
    "    out: list[str] = []\n",
    "    for key, value in args.items():\n",
    "        out.extend([f\"--{key}\", str(value)])\n",
    "    return out\n",
    "\n",
    "\n",
    "def run_and_capture(cmd: list[str], cwd: Path) -> tuple[int, list[str]]:\n",
    "    env = os.environ.copy()\n",
    "    pythonpath = env.get(\"PYTHONPATH\", \"\")\n",
    "    env[\"PYTHONPATH\"] = f\"{PROJECT_ROOT}{os.pathsep}{pythonpath}\" if pythonpath else str(PROJECT_ROOT)\n",
    "\n",
    "    print(\"$\", shlex.join(cmd))\n",
    "    proc = subprocess.Popen(\n",
    "        cmd,\n",
    "        cwd=str(cwd),\n",
    "        env=env,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "    )\n",
    "\n",
    "    lines: list[str] = []\n",
    "    assert proc.stdout is not None\n",
    "    for raw in proc.stdout:\n",
    "        print(raw, end=\"\")\n",
    "        lines.append(raw.rstrip(\"\\n\"))\n",
    "\n",
    "    rc = proc.wait()\n",
    "    return rc, lines\n",
    "\n",
    "\n",
    "def parse_learn_base_metrics(lines: list[str]) -> dict[str, float]:\n",
    "    out: dict[str, float] = {}\n",
    "    p_acc = re.compile(r\"train acc:\\s*([+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?),\\s*test acc:\\s*([+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?)\")\n",
    "    p_ap = re.compile(r\"train ap:\\s*([+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?),\\s*test ap:\\s*([+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?)\")\n",
    "    p_auc = re.compile(r\"train auc:\\s*([+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?),\\s*test auc:\\s*([+-]?\\d*\\.?\\d+(?:[eE][+-]?\\d+)?)\")\n",
    "\n",
    "    for line in lines:\n",
    "        m = p_acc.search(line)\n",
    "        if m:\n",
    "            out[\"train_acc\"] = float(m.group(1))\n",
    "            out[\"test_acc\"] = float(m.group(2))\n",
    "        m = p_ap.search(line)\n",
    "        if m:\n",
    "            out[\"train_ap\"] = float(m.group(1))\n",
    "            out[\"test_ap\"] = float(m.group(2))\n",
    "        m = p_auc.search(line)\n",
    "        if m:\n",
    "            out[\"train_auc\"] = float(m.group(1))\n",
    "            out[\"test_auc\"] = float(m.group(2))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def parse_testing_epoch_metrics(lines: list[str]) -> dict[str, float]:\n",
    "    testing_line = None\n",
    "    for line in reversed(lines):\n",
    "        if line.strip().startswith(\"Testing Epoch:\"):\n",
    "            testing_line = line\n",
    "            break\n",
    "\n",
    "    if testing_line is None:\n",
    "        return {}\n",
    "\n",
    "    out: dict[str, float] = {}\n",
    "    for part in testing_line.split(\"|\"):\n",
    "        if \":\" not in part:\n",
    "            continue\n",
    "        key, value = part.split(\":\", 1)\n",
    "        key = key.strip().lower().replace(\" \", \"_\")\n",
    "        value = value.strip()\n",
    "        try:\n",
    "            out[key] = float(value)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def save_metrics_and_logs(metrics_rows: list[dict], logs: dict[str, list[str]]) -> tuple[Path, Path]:\n",
    "    out_dir = PROJECT_ROOT / \"notebooks\" / \"runs\" / \"tempme_wikipedia_tgn\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_rows)\n",
    "    metrics_path = out_dir / f\"metrics_{ts}.csv\"\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "    logs_path = out_dir / f\"logs_{ts}.json\"\n",
    "    logs_path.write_text(json.dumps(logs, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    return metrics_path, logs_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ /Users/juliawenkmann/miniconda3/envs/graphs/bin/python /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/explainer/TempME/learn_base.py --base_type tgn --data wikipedia --gpu -1 --n_epoch 1 --bs 1024\n",
      "dataset:wikipedia, base_type model:tgn\n",
      "num of training instances: 79376\n",
      "num of batches per epoch: 78\n",
      "start 0 epoch\n",
      "\n",
      "  0%|          | 0/78 [00:00<?, ?it/s]\n",
      "  1%|▏         | 1/78 [00:22<28:35, 22.28s/it]\n",
      "  3%|▎         | 2/78 [00:42<26:32, 20.95s/it]\n",
      "  4%|▍         | 3/78 [00:58<23:45, 19.01s/it]\n",
      "  5%|▌         | 4/78 [01:16<22:28, 18.23s/it]\n",
      "  6%|▋         | 5/78 [01:49<28:45, 23.63s/it]\n",
      "  8%|▊         | 6/78 [02:38<38:40, 32.22s/it]\n",
      "  9%|▉         | 7/78 [03:21<42:35, 36.00s/it]\n",
      " 10%|█         | 8/78 [04:14<48:10, 41.30s/it]\n",
      " 12%|█▏        | 9/78 [04:58<48:24, 42.10s/it]\n"
     ]
    }
   ],
   "source": [
    "base_logs: list[str] = []\n",
    "base_metrics: dict[str, float] = {}\n",
    "\n",
    "base_cmd = [\n",
    "    PYTHON_BIN,\n",
    "    str(TEMP_ME_ROOT / \"learn_base.py\"),\n",
    "    \"--base_type\", BASE_TYPE,\n",
    "    \"--data\", DATASET_NAME,\n",
    "    *_cli_args(BASE_OVERRIDES),\n",
    "]\n",
    "\n",
    "if BASE_CKPT.exists() and not FORCE_RERUN_BASE:\n",
    "    print(f\"Skipping learn_base.py (checkpoint exists): {BASE_CKPT}\")\n",
    "else:\n",
    "    rc, base_logs = run_and_capture(base_cmd, TEMP_ME_ROOT)\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"learn_base.py failed with exit code {rc}\")\n",
    "\n",
    "base_metrics = parse_learn_base_metrics(base_logs)\n",
    "base_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_logs: list[str] = []\n",
    "expl_metrics: dict[str, float] = {}\n",
    "\n",
    "exp_cmd = [\n",
    "    PYTHON_BIN,\n",
    "    str(TEMP_ME_ROOT / \"temp_exp_main.py\"),\n",
    "    \"--base_type\", BASE_TYPE,\n",
    "    \"--data\", DATASET_NAME,\n",
    "    *_cli_args(EXPLAINER_OVERRIDES),\n",
    "]\n",
    "\n",
    "if EXPL_CKPT.exists() and not FORCE_RERUN_EXPLAINER:\n",
    "    print(f\"Skipping temp_exp_main.py (checkpoint exists): {EXPL_CKPT}\")\n",
    "else:\n",
    "    rc, expl_logs = run_and_capture(exp_cmd, TEMP_ME_ROOT)\n",
    "    if rc != 0:\n",
    "        raise RuntimeError(f\"temp_exp_main.py failed with exit code {rc}\")\n",
    "\n",
    "expl_metrics = parse_testing_epoch_metrics(expl_logs)\n",
    "expl_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows: list[dict[str, object]] = []\n",
    "if base_metrics:\n",
    "    rows.append({\"stage\": \"learn_base\", **base_metrics})\n",
    "if expl_metrics:\n",
    "    rows.append({\"stage\": \"temp_exp_main\", **expl_metrics})\n",
    "\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "display(metrics_df)\n",
    "\n",
    "logs_payload = {\n",
    "    \"learn_base\": base_logs,\n",
    "    \"temp_exp_main\": expl_logs,\n",
    "}\n",
    "metrics_path, logs_path = save_metrics_and_logs(rows, logs_payload)\n",
    "print(\"Saved metrics to:\", metrics_path)\n",
    "print(\"Saved logs to:\", logs_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
