{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Datasets\n",
    "## Setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using helpers from: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/notebooks/src\n",
      "PARENT_DIR       : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain\n",
      "RAW_DATA_FILE    : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/datasets/raw/wikipedia.csv\n",
      "DATASET_DIR      : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/datasets/processed/wikipedia\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "def _add_notebooks_src_to_path():\n",
    "    here = Path.cwd().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        candidate = p / \"notebooks\" / \"src\"\n",
    "        if candidate.is_dir():\n",
    "            if str(candidate) not in sys.path:\n",
    "                sys.path.insert(0, str(candidate))\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Could not find 'notebooks/src' from current working directory.\")\n",
    "\n",
    "print(\"Using helpers from:\", _add_notebooks_src_to_path())\n",
    "\n",
    "from constants import (\n",
    "    REPO_ROOT, PKG_DIR, RAW_DATA_DIR, PROCESSED_DATA_DIR, MODELS_ROOT, ensure_repo_importable\n",
    ")\n",
    "dataset_name = \"wikipedia\"   # e.g., \"uci\"\n",
    "bipartite = True\n",
    "RAW_DATA_FILE = RAW_DATA_DIR / f\"{dataset_name}.csv\"\n",
    "DATASET_DIR = PROCESSED_DATA_DIR / dataset_name\n",
    "DATASET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"PARENT_DIR       :\", REPO_ROOT)\n",
    "print(\"RAW_DATA_FILE    :\", RAW_DATA_FILE)\n",
    "print(\"DATASET_DIR      :\", DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to running the experiments is the preparation of the datasets. The datasets can be downloaded from the \n",
    "following sites:\n",
    "\n",
    "- [Wikipedia](http://snap.stanford.edu/jodie/#datasets)\n",
    "- [UCI-Messages/UCI-Forums](https://toreopsahl.com/datasets/)\n",
    "\n",
    "For Wikipedia do: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      " 11  533M   11 60.7M    0     0  5894k      0  0:01:32  0:00:10  0:01:22 10.6M 0  0:06:06  0:00:05  0:06:01 1766k     0  3691k      0  0:02:28  0:00:08  0:02:20 5880k^C\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://snap.stanford.edu/jodie/wikipedia.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2258M  100 2258M    0     0  4083k      0  0:09:26  0:09:26 --:--:-- 6002k:00  0:00:37  0:06:23 6176k58M   12  292M    0     0  4378k      0  0:08:48  0:01:08  0:07:40 1939k13 2258M   13  298M    0     0  4273k      0  0:09:01  0:01:11  0:07:50 1991k58M   13  302M    0     0  4053k      0  0:09:30  0:01:16  0:08:14  862k    0  0:10:06  0:01:23  0:08:43 1341k   0  0:10:27  0:01:34  0:08:53 2685k04k      0  0:12:25  0:02:21  0:10:04 1327k0     0  3078k      0  0:12:31  0:02:23  0:10:08 1206k77k      0  0:13:23  0:02:53  0:10:30 2293k 0:13:24  0:02:57  0:10:27 2710k    0  3390k      0  0:11:22  0:03:35  0:07:47 6299k0     0  3481k      0  0:11:04  0:03:42  0:07:22 6194k  0  3521k      0  0:10:56  0:03:45  0:07:11 6312kk      0  0:10:29  0:03:59  0:06:30 5713k4k      0  0:10:01  0:04:18  0:05:43 6347k 0:09:26  0:04:46  0:04:40 6373kk      0  0:09:24  0:04:50  0:04:34 5625k0  0:09:11  0:05:06  0:04:05 5760k06k      0  0:08:57  0:05:24  0:03:33 6268k 0:08:55  0:05:26  0:03:29 6433k  0  0:08:52  0:05:29  0:03:23 6457k58M   63 1436M    0     0  4348k      0  0:08:51  0:05:38  0:03:13 3747k7  0:06:14  0:03:03 2318k8M   67 1526M    0     0  4118k      0  0:09:21  0:06:19  0:03:02 1863k 0  0:09:42  0:06:39  0:03:03  666k   0  0:10:24  0:07:26  0:02:58 1890k65k      0  0:10:30  0:07:47  0:02:43 3285k40k      0  0:10:18  0:08:12  0:02:06 6304k0:10:09  0:08:23  0:01:46 6214k0 6303k43  0:01:10 6268k3913k      0  0:09:50  0:08:47  0:01:03 6347k 0  3955k      0  0:09:44  0:08:56  0:00:48 6304k    0  0:09:33  0:09:14  0:00:19 6485k:19  0:00:11 6409k\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://snap.stanford.edu/jodie/reddit.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For UCI-Messages do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl http://opsahl.co.uk/tnet/datasets/OCnodeslinks.txt > UCI-Messages.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw dataset files should have the same format as the Wikipedia dataset; that is: \n",
    "\n",
    "- First column: Source node ids\n",
    "- Second column: Target node ids\n",
    "- Third column: UNIX timestamp\n",
    "- Fourth column: State label (not necessary for link prediction task)\n",
    "- Fifth column and onwards: Comma seperated list of edge features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat UCI-Messages:\n",
    "The UCI datasets do not have this form by default. To make the conversion easier, the \n",
    "[format_uci_data.py](./format_uci_data.py) is provided. First download the dataset to a file from the website \n",
    "mentioned above. Then use the script to convert the downloaded file to an appropriate .csv file by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/UCI-Messages.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m outp\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# --- Read raw ---\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Original script uses sep=\" \". If your file has variable whitespace, consider sep=r\"\\s+\", engine=\"python\".\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(inp, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# --- Transform ---\u001b[39;00m\n\u001b[1;32m     14\u001b[0m raw_data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/graphs/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/UCI-Messages.txt'"
     ]
    }
   ],
   "source": [
    "input_path = \"path/to/UCI-Messages.txt\"\n",
    "output_path = RAW_DATA_DIR / \"ucim.csv\"\n",
    "\n",
    "inp = Path(input_path)\n",
    "outp = Path(output_path)\n",
    "outp.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Read raw ---\n",
    "# Original script uses sep=\" \". If your file has variable whitespace, consider sep=r\"\\s+\", engine=\"python\".\n",
    "raw_data = pd.read_csv(inp, sep=\" \", header=None)\n",
    "\n",
    "# --- Transform ---\n",
    "raw_data.columns = ['timestamp', 'item_id', 'user_id', 'state_label']\n",
    "raw_data['timestamp'] = pd.to_datetime(raw_data['timestamp'])\n",
    "raw_data['timestamp'] = (raw_data['timestamp'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "reordered_data = raw_data[['user_id', 'item_id', 'timestamp', 'state_label']]\n",
    "\n",
    "# --- Save ---\n",
    "reordered_data.to_csv(outp, index=False)\n",
    "print(f\"Reformatted file saved to {outp}\")\n",
    "\n",
    "# (Optional) Peek at the first few rows\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(reordered_data.head())\n",
    "except Exception:\n",
    "    print(reordered_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place the correctly formatted datasets as .csv files in the [/resources/datasets/raw](./resources/datasets/raw) directory.\n",
    "\n",
    "To prepare the raw datasets for the usage we have to preprocess the files:\n",
    "\n",
    "Wikipedia/UCI-Forums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing…\n",
      "Dataset wikipedia has been processed and will be saved.\n",
      "\n",
      "Dataset information:\n",
      "Edge feature shape: (157474, 172)\n",
      "Node feature shape: (9227, 172)\n",
      "Successfully saved the preprocessed dataset to /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/datasets/processed/wikipedia\n",
      "\n",
      " Preprocessing finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preprocess_script = Path(PKG_DIR/\"data/preprocess_dataset.py\")\n",
    "py = sys.executable\n",
    "cmd = [\n",
    "    py,\n",
    "    preprocess_script,\n",
    "    \"-f\", str(RAW_DATA_FILE),\n",
    "    \"-t\", str(DATASET_DIR),\n",
    "]\n",
    "if bipartite:\n",
    "    cmd.append(\"--bipartite\")\n",
    "\n",
    "# Ensure Python can import the repo package when the script runs\n",
    "env = os.environ.copy()\n",
    "py_paths = [str(PKG_DIR)]\n",
    "if env.get(\"PYTHONPATH\"):\n",
    "    py_paths.append(env[\"PYTHONPATH\"])\n",
    "env[\"PYTHONPATH\"] = os.pathsep.join(py_paths)\n",
    "\n",
    "print(\"\\nExecuting…\")\n",
    "subprocess.run(cmd, cwd=str(preprocess_script.parent), env=env, check=True)\n",
    "print(\"\\n Preprocessing finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
