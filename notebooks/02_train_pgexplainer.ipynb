{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the PGExplainer model\n",
    "T-GNNExplainer relies on a pretrained navigator that is realized as dynamic adaptation of PGExplainer. \n",
    "Thus, this navigator component has to be trained prior to evaluating T-GNNExplainer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using helpers from: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/notebooks/src\n",
      "utils.utils   -> ModuleSpec(name='utils.utils', loader=<_frozen_importlib_external.SourceFileLoader object at 0x34377d7d0>, origin='/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/models/tgn/utils/utils.py')\n",
      "modules.memory-> ModuleSpec(name='modules.memory', loader=<_frozen_importlib_external.SourceFileLoader object at 0x34377e3d0>, origin='/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/models/tgn/modules/memory.py')\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_tgnn_wrapper' from 'time_to_explain.models.wrapper' (/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/time_to_explain/models/wrapper.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodules.memory->\u001b[39m\u001b[38;5;124m\"\u001b[39m, iu\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodules.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 5) Now this import should work without the previous error\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime_to_explain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_pgexplainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     42\u001b[0m     train_pgexplainer\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREPO_ROOT        :\u001b[39m\u001b[38;5;124m\"\u001b[39m, REPO_ROOT)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPKG_DIR          :\u001b[39m\u001b[38;5;124m\"\u001b[39m, PKG_DIR)\n",
      "File \u001b[0;32m~/Documents/CodingProjects/master_thesis/time_to_explain/time_to_explain/explainer/train_pgexplainer.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Iterable, Optional\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# -- Your variable-based wrapper factory (adjust if you named it differently)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime_to_explain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_tgnn_wrapper\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime_to_explain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madapter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtgn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_data_object  \u001b[38;5;66;03m# helper used to build neighbor finder\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# PGExplainer bits\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_tgnn_wrapper' from 'time_to_explain.models.wrapper' (/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/time_to_explain/models/wrapper.py)"
     ]
    }
   ],
   "source": [
    "# Find and add `notebooks/src` to sys.path, no matter where the notebook lives.\n",
    "from pathlib import Path\n",
    "import sys, importlib\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def _add_notebooks_src_to_path():\n",
    "    here = Path.cwd().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        candidate = p / \"notebooks\" / \"src\"\n",
    "        if candidate.is_dir():\n",
    "            if str(candidate) not in sys.path:\n",
    "                sys.path.insert(0, str(candidate))\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Could not find 'notebooks/src' from current working directory.\")\n",
    "\n",
    "print(\"Using helpers from:\", _add_notebooks_src_to_path())\n",
    "\n",
    "from constants import (\n",
    "    REPO_ROOT, PKG_DIR, RESOURCES_DIR, PROCESSED_DATA_DIR, MODELS_ROOT, TGN_SUBMODULE_ROOT, ensure_repo_importable, get_last_checkpoint\n",
    ")\n",
    "ensure_repo_importable()\n",
    "from device import pick_device\n",
    "\n",
    "for p in (str(TGN_SUBMODULE_ROOT), str(REPO_ROOT), str(PKG_DIR)):\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "# 2) If your notebook already imported `utils`, remove it to avoid collision\n",
    "if \"utils\" in sys.modules:\n",
    "    del sys.modules[\"utils\"]\n",
    "\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "# 4) (Optional) sanity check that TGN's local packages resolve\n",
    "import importlib.util as iu\n",
    "print(\"utils.utils   ->\", iu.find_spec(\"utils.utils\"))\n",
    "print(\"modules.memory->\", iu.find_spec(\"modules.memory\"))\n",
    "\n",
    "# 5) Now this import should work without the previous error\n",
    "from time_to_explain.explainer.train_pgexplainer import (\n",
    "    train_pgexplainer\n",
    ")\n",
    "\n",
    "print(\"REPO_ROOT        :\", REPO_ROOT)\n",
    "print(\"PKG_DIR          :\", PKG_DIR)\n",
    "print(\"RESOURCES_DIR    :\", RESOURCES_DIR)\n",
    "print(\"PROCESSED_DATA_DIR:\", PROCESSED_DATA_DIR)\n",
    "print(\"MODELS_ROOT      :\", MODELS_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting\n",
    "\n",
    "Replace ``MODEL-TYPE`` with the type of the model you want to evaluate, e.g., 'TGAT' or 'TGN'.\n",
    "\n",
    "Replace ``DATASET-NAME`` with the name of the dataset on which you want to train the PGExplainer model, e.g., 'uci', \n",
    "'wikipedia', etc.\n",
    "\n",
    "Only provide the ``--bipartite`` flag if the underlying dataset is a bipartite graph (Wikipedia/UCI-Forums), else\n",
    "omit the ``--bipartite`` flag from the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE = \"TGAT\"\n",
    "DATASET_NAME = \"wikipedia\"\n",
    "BIPARTITE = True\n",
    "\n",
    "DIRECTED = False\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "MODEL_PATH = MODELS_ROOT / DATASET_NAME\n",
    "CHECKPOINT_PATH = MODEL_PATH / 'checkpoints/'\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    os.mkdir(CHECKPOINT_PATH)\n",
    "LAST_CHECKPOINT = get_last_checkpoint(CHECKPOINT_PATH,MODEL_TYPE, DATASET_NAME)    \n",
    "DEVICE = pick_device(\"auto\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train PGExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pgexplainer(\n",
    "    *,\n",
    "    dataset_name: str,                 # e.g., \"wikipedia\"\n",
    "    model_type: str,                   # \"TGN\" or \"TGAT\" (or \"TTGN\" if you kept that label)\n",
    "    epochs: int = 100,\n",
    "    learning_rate: float = 1e-4,\n",
    "    batch_size: int = 16,\n",
    "    directed: bool = False,\n",
    "    bipartite: bool = False,\n",
    "    device: str = \"auto\",              # \"auto\" | \"cpu\" | \"cuda\" | \"mps\"\n",
    "    cuda: bool = True,                 # legacy flag still supported by your device selector\n",
    "    candidates_size: int = 30,         # kept for compatibility (some wrappers read it)\n",
    "    # Base model checkpoint to explain:\n",
    "    tgn_checkpoint: str | Path | None = None,\n",
    "    # Where to place the PGExplainer outputs:\n",
    "    out_dir: str | Path | None = None,  # default: MODELS_ROOT/<dataset>/pg_explainer\n",
    "    # Roots (normally from constants.py)\n",
    "    models_root: str | Path = None,\n",
    "    processed_root: str | Path = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train PGExplainer in-notebook with variables only.\n",
    "\n",
    "    Returns:\n",
    "      explainer, wrapper, out_dir (Path)\n",
    "    \"\"\"\n",
    "    dataset_dir = Path(processed_root) / dataset_name\n",
    "    if not dataset_dir.exists():\n",
    "        raise FileNotFoundError(f\"Processed dataset not found: {dataset_dir}\")\n",
    "\n",
    "    out_dir = Path(out_dir) if out_dir else Path(models_root) / dataset_name / \"pg_explainer\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Resolve base model checkpoint (file)\n",
    "    resume_file = get_last_checkpoint(\n",
    "        models_root=models_root,\n",
    "        dataset=dataset_name,\n",
    "        model_type=model_type,\n",
    "        tgn_checkpoint=tgn_checkpoint,\n",
    "    )\n",
    "    if resume_file is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not resolve a base TGN/TGAT checkpoint to explain.\\n\"\n",
    "            f\"Searched under: {models_root}/{dataset_name}/checkpoints and \"\n",
    "            f\"{models_root}/{dataset_name}/{model_type}-{dataset_name}.pth\\n\"\n",
    "            \"Pass `tgn_checkpoint` explicitly if needed.\"\n",
    "        )\n",
    "\n",
    "    # Build the wrapper (loads weights from resume_file)\n",
    "    wrapper = create_tgnn_wrapper(\n",
    "        model_type=model_type,\n",
    "        dataset_dir=str(dataset_dir),\n",
    "        directed=directed,\n",
    "        bipartite=bipartite,\n",
    "        device=device,\n",
    "        cuda=cuda,\n",
    "        update_memory_at_start=False,\n",
    "        checkpoint_path=resume_file,   # <-- FILE to load\n",
    "    )\n",
    "\n",
    "    # Optional compatibility knob (some code reads this)\n",
    "    try:\n",
    "        setattr(wrapper, \"explanation_candidates_size\", candidates_size)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Make sure eval neighbor finder exists\n",
    "    _ensure_full_neighbor_finder(wrapper)\n",
    "\n",
    "    # Build PGExplainer\n",
    "    embedding = StaticEmbedding(wrapper.dataset, wrapper)\n",
    "    explainer = TPGExplainer(wrapper, embedding, device=wrapper.device)\n",
    "\n",
    "    # Train PGExplainer\n",
    "    model_name = getattr(wrapper, \"name\", getattr(wrapper, \"model_name\", model_type))\n",
    "    print(f\"Training PGExplainer for base model '{model_name}' on dataset '{dataset_name}'\")\n",
    "    print(\"dataset dir   :\", dataset_dir)\n",
    "    print(\"output dir    :\", out_dir)\n",
    "    print(\"base checkpoint:\", resume_file)\n",
    "    print(\"device        :\", wrapper.device)\n",
    "    print(\"epochs        :\", epochs, \"| lr:\", learning_rate, \"| batch:\", batch_size)\n",
    "\n",
    "    explainer.train(\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        model_name=model_name,\n",
    "        save_directory=str(out_dir),\n",
    "    )\n",
    "\n",
    "    return explainer, wrapper, out_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer, base_wrapper, pg_dir = train_pgexplainer(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    model_type=MODEL_TYPE,\n",
    "    epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    directed=DIRECTED,\n",
    "    bipartite=BIPARTITE,             # set if your dataset is bipartite\n",
    "    device=DEVICE,              # \"cuda\"/\"cpu\"/\"mps\" also fine               \n",
    "    # tgn_checkpoint=\"/path/to/TGAT-wikipedia-19.pth\",  # optional explicit file\n",
    "    # out_dir=\"/custom/output/path\",                    # optional custom folder\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
