{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training mdoels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T22:27:20.225260Z",
     "iopub.status.busy": "2025-10-26T22:27:20.224814Z",
     "iopub.status.idle": "2025-10-26T22:27:21.281528Z",
     "shell.execute_reply": "2025-10-26T22:27:21.281134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using helpers from: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/notebooks/src\n",
      "utils.utils   -> ModuleSpec(name='utils.utils', loader=<_frozen_importlib_external.SourceFileLoader object at 0x1119453d0>, origin='/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/models/tgn/TTGN/utils/utils.py')\n",
      "modules.memory-> ModuleSpec(name='modules.memory', loader=<_frozen_importlib_external.SourceFileLoader object at 0x111946050>, origin='/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/models/tgn/TTGN/modules/memory.py')\n",
      "REPO_ROOT        : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain\n",
      "PKG_DIR          : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/time_to_explain\n",
      "RESOURCES_DIR    : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources\n",
      "PROCESSED_DATA_DIR: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/datasets/processed\n",
      "MODELS_ROOT      : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/models\n"
     ]
    }
   ],
   "source": [
    "# Find and add `notebooks/src` to sys.path, no matter where the notebook lives.\n",
    "from pathlib import Path\n",
    "import sys, importlib\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def _add_notebooks_src_to_path():\n",
    "    here = Path.cwd().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        candidate = p / \"notebooks\" / \"src\"\n",
    "        if candidate.is_dir():\n",
    "            if str(candidate) not in sys.path:\n",
    "                sys.path.insert(0, str(candidate))\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Could not find 'notebooks/src' from current working directory.\")\n",
    "\n",
    "print(\"Using helpers from:\", _add_notebooks_src_to_path())\n",
    "\n",
    "from src.constants import (\n",
    "    REPO_ROOT, PKG_DIR, RESOURCES_DIR, PROCESSED_DATA_DIR, MODELS_ROOT, TGN_SUBMODULE_ROOT, ensure_repo_importable, get_last_checkpoint\n",
    ")\n",
    "ensure_repo_importable()\n",
    "from src.device import pick_device\n",
    "\n",
    "for p in (str(TGN_SUBMODULE_ROOT), str(REPO_ROOT), str(PKG_DIR)):\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "# 2) If your notebook already imported `utils`, remove it to avoid collision\n",
    "if \"utils\" in sys.modules:\n",
    "    del sys.modules[\"utils\"]\n",
    "\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "# 4) (Optional) sanity check that TGN's local packages resolve\n",
    "import importlib.util as iu\n",
    "print(\"utils.utils   ->\", iu.find_spec(\"utils.utils\"))\n",
    "print(\"modules.memory->\", iu.find_spec(\"modules.memory\"))\n",
    "\n",
    "# 5) Now this import should work without the previous error\n",
    "from time_to_explain.models.create_wrapper import create_tgn_wrapper, create_wrapper, create_tgat_wrapper\n",
    "from time_to_explain.data.data import create_dataset\n",
    "\n",
    "print(\"REPO_ROOT        :\", REPO_ROOT)\n",
    "print(\"PKG_DIR          :\", PKG_DIR)\n",
    "print(\"RESOURCES_DIR    :\", RESOURCES_DIR)\n",
    "print(\"PROCESSED_DATA_DIR:\", PROCESSED_DATA_DIR)\n",
    "print(\"MODELS_ROOT      :\", MODELS_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T22:27:21.282889Z",
     "iopub.status.busy": "2025-10-26T22:27:21.282779Z",
     "iopub.status.idle": "2025-10-26T22:27:21.293782Z",
     "shell.execute_reply": "2025-10-26T22:27:21.293432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "MODEL_TYPE = \"TGAT\"\n",
    "DATASET_NAME = \"wikipedia\"\n",
    "BIPARTITE = True\n",
    "DIRECTED = False\n",
    "EPOCHS = 30\n",
    "\n",
    "MODEL_PATH = MODELS_ROOT / DATASET_NAME\n",
    "CHECKPOINT_PATH = MODEL_PATH / 'checkpoints/'\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    os.mkdir(CHECKPOINT_PATH)\n",
    "LAST_CHECKPOINT = get_last_checkpoint(CHECKPOINT_PATH,MODEL_TYPE, DATASET_NAME)    \n",
    "DEVICE = pick_device(\"auto\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-26T22:27:21.294901Z",
     "iopub.status.busy": "2025-10-26T22:27:21.294830Z",
     "iopub.status.idle": "2025-10-26T22:31:56.482263Z",
     "shell.execute_reply": "2025-10-26T22:31:56.481583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TGATWrapper...\n",
      "The dataset has 157474 interactions, involving 9227 different nodes\n",
      "The training dataset has 76333 interactions, involving 6006 different nodes\n",
      "The validation dataset has 23621 interactions, involving 3256 different nodes\n",
      "The test dataset has 23621 interactions, involving 3564 different nodes\n",
      "The new node validation dataset has 12117 interactions, involving 2098 different nodes\n",
      "The new node test dataset has 12132 interactions, involving 2491 different nodes\n",
      "922 nodes were used for the inductive testing, i.e. are never seen during training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TGNNWrapper:num of training instances: 76333\n",
      "INFO:TGNNWrapper:num of batches per epoch: 2386\n",
      "INFO:TGNNWrapper:start 0 epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e60b4b2da548d386ae0c7083051b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/2386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TGNNWrapper:epoch: 0 took 277.14s\n",
      "INFO:TGNNWrapper:Epoch mean loss: 1.0313809098611832\n",
      "INFO:TGNNWrapper:val auc: 0.9131933461603519, new node val auc: 0.9071093702088906\n",
      "INFO:TGNNWrapper:val ap: 0.9224124375262739, new node val ap: 0.9150364530376641\n",
      "INFO:TGNNWrapper:val acc: 0.8323917456021651, new node val acc: 0.8280778835280815\n",
      "INFO:TGNNWrapper:start 1 epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d64eaad4e44cf2922dc0d16a26d930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TGNNWrapper:epoch: 1 took 184.51s\n",
      "INFO:TGNNWrapper:Epoch mean loss: 0.8177597169816644\n",
      "INFO:TGNNWrapper:val auc: 0.907937774864682, new node val auc: 0.9046663045990164\n",
      "INFO:TGNNWrapper:val ap: 0.9208947215709856, new node val ap: 0.9162048060978438\n",
      "INFO:TGNNWrapper:val acc: 0.8226023342354534, new node val acc: 0.8204509046362609\n",
      "INFO:TGNNWrapper:start 2 epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2016853f288f485e977afe09a23f2ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/2386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TGNNWrapper:epoch: 2 took 189.42s\n",
      "INFO:TGNNWrapper:Epoch mean loss: 0.832847383700792\n",
      "INFO:TGNNWrapper:val auc: 0.8944951211518944, new node val auc: 0.8915466020469863\n",
      "INFO:TGNNWrapper:val ap: 0.9093140847806388, new node val ap: 0.9060224879388133\n",
      "INFO:TGNNWrapper:val acc: 0.805049052774019, new node val acc: 0.8050124073376053\n",
      "INFO:TGNNWrapper:start 3 epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65307e104e24806ad8f8211d0410ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/2386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from time_to_explain.models.adapter.tgn_unified import make_trainer\n",
    "\n",
    "dataset_dir = Path(PROCESSED_DATA_DIR) / DATASET_NAME\n",
    "if not dataset_dir.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Processed dataset directory not found: {dataset_dir}\\n\"\n",
    "        f\"Set PROCESSED_DATA_DIR in constants.py or fix dataset_name.\")\n",
    "\n",
    "dataset = create_dataset(dataset_dir=dataset_dir, directed=DIRECTED, bipartite=BIPARTITE)\n",
    "trainer = make_trainer(dataset=dataset, model_type=MODEL_TYPE, device=DEVICE, cuda=True)\n",
    "trainer.train_model(\n",
    "    epochs=EPOCHS,\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    model_path=MODEL_PATH,\n",
    "    results_path=f\"{MODEL_PATH}/results.pkl\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
