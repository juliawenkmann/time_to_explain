{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate Explainers\n",
    "\n",
    "With all these prerequisites out of the way you can now run the experiments themselves. The experiments are run for each\n",
    "explanation method (T-GNNExplainer, GreDyCF, CoDy), for each dataset, for each correct/incorrect setting \n",
    "(correct predictions only/incorrect predictions only), and for each selection policy (random, temporal, spatio-temporal, \n",
    "local-gradient) separately. For convenience, all selection strategies can be automatically evaluated in parallel from a \n",
    "single script. An additional feature of the evaluation is that it can be interrupted by Keyboard Interruption or by the\n",
    "maximum processing time. When the evaluation is interrupted before it is finished, the intermediary results are saved. \n",
    "The evaluation automatically resumes from intermediary results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using helpers from: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/notebooks/src\n",
      "utils.utils   -> ModuleSpec(name='utils.utils', loader=<_frozen_importlib_external.SourceFileLoader object at 0x307d6fa90>, origin='/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/time_to_explain/utils/utils.py')\n",
      "modules.memory-> ModuleSpec(name='modules.memory', loader=<_frozen_importlib_external.SourceFileLoader object at 0x10699f190>, origin='/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/models/tgn/modules/memory.py')\n",
      "REPO_ROOT        : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain\n",
      "PKG_DIR          : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/time_to_explain\n",
      "RESOURCES_DIR    : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources\n",
      "PROCESSED_DATA_DIR: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/datasets/processed\n",
      "MODELS_ROOT      : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/models\n"
     ]
    }
   ],
   "source": [
    "# Find and add `notebooks/src` to sys.path, no matter where the notebook lives.\n",
    "from pathlib import Path\n",
    "import sys, importlib\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def _add_notebooks_src_to_path():\n",
    "    here = Path.cwd().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        candidate = p / \"notebooks\" / \"src\"\n",
    "        if candidate.is_dir():\n",
    "            if str(candidate) not in sys.path:\n",
    "                sys.path.insert(0, str(candidate))\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Could not find 'notebooks/src' from current working directory.\")\n",
    "\n",
    "print(\"Using helpers from:\", _add_notebooks_src_to_path())\n",
    "\n",
    "from constants import (\n",
    "    REPO_ROOT, PKG_DIR, RESOURCES_DIR, PROCESSED_DATA_DIR, MODELS_ROOT, TGN_SUBMODULE_ROOT, ensure_repo_importable, get_last_checkpoint\n",
    ")\n",
    "ensure_repo_importable()\n",
    "from device import pick_device\n",
    "\n",
    "for p in (str(TGN_SUBMODULE_ROOT), str(REPO_ROOT), str(PKG_DIR)):\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "# 2) If your notebook already imported `utils`, remove it to avoid collision\n",
    "if \"utils\" in sys.modules:\n",
    "    del sys.modules[\"utils\"]\n",
    "\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "# 4) (Optional) sanity check that TGN's local packages resolve\n",
    "import importlib.util as iu\n",
    "print(\"utils.utils   ->\", iu.find_spec(\"utils.utils\"))\n",
    "print(\"modules.memory->\", iu.find_spec(\"modules.memory\"))\n",
    "\n",
    "# 5) Now this import should work without the previous error\n",
    "from time_to_explain.adapters import*\n",
    "\n",
    "print(\"REPO_ROOT        :\", REPO_ROOT)\n",
    "print(\"PKG_DIR          :\", PKG_DIR)\n",
    "print(\"RESOURCES_DIR    :\", RESOURCES_DIR)\n",
    "print(\"PROCESSED_DATA_DIR:\", PROCESSED_DATA_DIR)\n",
    "print(\"MODELS_ROOT      :\", MODELS_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting:\n",
    "\n",
    "Replace ``MODEL-TYPE`` with the type of the model you want to evaluate, e.g., 'TGAT' or 'TGN'.\n",
    "\n",
    "Replace ``DATASET-NAME`` with the name of the dataset on which you want to train the PGExplainer model, e.g., 'uci', \n",
    "'wikipedia', etc.\n",
    "\n",
    "Replace ``EXPLAINER-NAME`` with the explainer you want to evaluate. Options are ``tgnnexplainer``, ``greedy``, ``cody``.\n",
    "\n",
    "Replace ``SELECTION-NAME`` with the selection policy that you want to evaluate. The options are ``random``, \n",
    "``temporal``, ``spatio-temporal``, ``local-gradient``, and ``all``. Use the ``all`` option to efficiently evaluate the\n",
    "different selection strategies with caching between selection strategies.\n",
    "**Do not provide a `SELECTION-NAME`` argument when evaluating T-GNNExplainer**\n",
    "\n",
    "Replace ``TIME-LIMIT`` with an integer number that sets a limit on the maximum time that the evaluation runs before \n",
    "concluding in minutes. The evaluation can be resumed from that state at a later time.\n",
    "\n",
    "Only set ``bipartite = True``  if the underlying dataset is a bipartite graph (Wikipedia/UCI-Forums).\n",
    "\n",
    "As an example, to run the evaluation of CoDy for all selection strategies, with a time limit of 240 minutes and the\n",
    "bipartite wikipedia dataset, the following command is used:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE = \"TGAT\"\n",
    "DATASET_NAME = \"wikipedia\"\n",
    "EXPLAINER = \"cody\"\n",
    "SELECTION_NAME = \"all\"\n",
    "TIME_LIMIT = 240\n",
    "BIPARTITE = True\n",
    "\n",
    "DIRECTED = False\n",
    "EPOCHS = 10\n",
    "\n",
    "MODEL_PATH = MODELS_ROOT / DATASET_NAME\n",
    "CHECKPOINT_PATH = MODEL_PATH / 'checkpoints/'\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    os.mkdir(CHECKPOINT_PATH)\n",
    "LAST_CHECKPOINT = get_last_checkpoint(CHECKPOINT_PATH,MODEL_TYPE, DATASET_NAME)    \n",
    "DEVICE = pick_device(\"auto\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using REPO_ROOT / ROOT_DIR: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain\n",
      "Using REPO_ROOT / ROOT_DIR: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain\n",
      "Using REPO_ROOT / ROOT_DIR: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain\n",
      "/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/models/wikipedia/checkpoints/tgn_wikipedia_best.pth\n"
     ]
    }
   ],
   "source": [
    "# --- core imports from your project ---\n",
    "import torch\n",
    "from time_to_explain.data.legacy.tg_dataset import load_tg_dataset, load_explain_idx\n",
    "from submodules.explainer.tgnnexplainer.tgnnexplainer.xgraph.dataset.utils_dataset import construct_tgat_neighbor_finder\n",
    "from submodules.models.tgat.module import TGAN\n",
    "from submodules.models.tgn.model.tgn import TGN\n",
    "from submodules.explainer.tgnnexplainer.tgnnexplainer.xgraph.models.ext.tgn.utils.data_processing import compute_time_statistics\n",
    "\n",
    "# --- unified framework imports ---\n",
    "from time_to_explain.core.runner import EvaluationRunner, EvalConfig\n",
    "from time_to_explain.adapters.subgraphx_tg_adapter import (\n",
    "    SubgraphXTGAdapter, SubgraphXTGAdapterConfig\n",
    ")\n",
    "from time_to_explain.adapters.tg_model_adapter import TemporalGNNModelAdapter\n",
    "from time_to_explain.extractors.tg_event_candidates_extractor import TGEventCandidatesExtractor\n",
    "import time_to_explain.metrics.sparsity\n",
    "import time_to_explain.metrics.fidelity\n",
    "# --- utilities ---\n",
    "import os, pandas as pd\n",
    "\n",
    "# ---- your notebook knobs ----\n",
    "dataset_name = \"wikipedia\"          # e.g., \"wikipedia\", \"reddit\", ...\n",
    "model_name   = \"tgn\"                # \"tgn\" or \"tgat\"\n",
    "use_gpu      = torch.cuda.is_available()\n",
    "device_id    = 0\n",
    "\n",
    "# event list file (1-based indices)\n",
    "explain_idx_csv = str(\n",
    "    RESOURCES_DIR / \"explainer\"  / \"explain_index\" / f\"{dataset_name}.csv\"\n",
    ")\n",
    "\n",
    "# backbone checkpoint path (same pattern as your Hydra pipeline)\n",
    "ckpt_path = str(\n",
    "    RESOURCES_DIR / \"models\" /  f\"{dataset_name}\" / \"checkpoints\" / f\"{model_name}_{dataset_name}_best.pth\"\n",
    ")\n",
    "print(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#Dataset: wikipedia, #Users: 8227, #Items: 1000, #Interactions: 157474, #Timestamps: 152757\n",
      "#node feats shape: (9228, 172), #edge feats shape: (157475, 172)\n",
      "\n",
      "Backbone ready on mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/xf0yhfbs02ggfm7n1l5g_cnm0000gn/T/ipykernel_7189/3956096970.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "events, edge_feats, node_feats = load_tg_dataset(dataset_name)\n",
    "\n",
    "# Build the backbone model\n",
    "if model_name == \"tgat\":\n",
    "    ngh_finder = construct_tgat_neighbor_finder(events)\n",
    "    backbone = TGAN(\n",
    "        ngh_finder,\n",
    "        node_feats,\n",
    "        edge_feats,\n",
    "        device=DEVICE,\n",
    "        attn_mode=\"prod\",\n",
    "        use_time=\"time\",\n",
    "        agg_method=\"attn\",\n",
    "        num_layers=2,\n",
    "        n_head=4,\n",
    "        null_idx=0,\n",
    "        num_neighbors=20,\n",
    "        drop_out=0.1,\n",
    "    )\n",
    "elif model_name == \"tgn\":\n",
    "    m_src, s_src, m_dst, s_dst = compute_time_statistics(events.u.values, events.i.values, events.ts.values)\n",
    "    ngh_finder = construct_tgat_neighbor_finder(events)   # your utils often reuse the same NF\n",
    "    backbone = TGN(\n",
    "        ngh_finder,\n",
    "        node_feats,\n",
    "        edge_feats,\n",
    "        device=DEVICE,\n",
    "        n_layers=2,\n",
    "        n_heads=2,\n",
    "        dropout=0.1,\n",
    "        use_memory=False,\n",
    "        forbidden_memory_update=False,\n",
    "        memory_update_at_start=True,\n",
    "        message_dimension=100,\n",
    "        memory_dimension=500,\n",
    "        embedding_module_type=\"graph_attention\",\n",
    "        message_function=\"mlp\",\n",
    "        mean_time_shift_src=0,\n",
    "        std_time_shift_src=1,\n",
    "        mean_time_shift_dst=0,\n",
    "        std_time_shift_dst=1,\n",
    "        n_neighbors=None,\n",
    "        aggregator_type=\"last\",\n",
    "        memory_updater_type=\"gru\",\n",
    "        use_destination_embedding_in_message=False,\n",
    "        use_source_embedding_in_message=False,\n",
    "        dyrep=False,\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError(model_name)\n",
    "\n",
    "# Load backbone weights\n",
    "state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "_ = backbone.load_state_dict(state_dict, strict=False)\n",
    "_ = backbone.to(DEVICE).eval()\n",
    "print(\"Backbone ready on\", DEVICE)\n",
    "# Wrap backbone with ModelProtocol adapter (adds predict_proba & masking)\n",
    "model = TemporalGNNModelAdapter(backbone, events, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Extractor and Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractor creates a stable candidate edge order for each event (needed for fair metrics)\n",
    "extractor = TGEventCandidatesExtractor(\n",
    "    model=model,\n",
    "    events=events,\n",
    "    threshold_num=50,               # same as your SubgraphX config\n",
    "    keep_order=\"last-N-then-sort\",  # matches SubgraphX-TG's pattern\n",
    ")\n",
    "\n",
    "# Adapter wraps your existing SubgraphX‑TG with the same knobs you use in Hydra\n",
    "adapter_cfg = SubgraphXTGAdapterConfig(\n",
    "    model_name=model_name,\n",
    "    dataset_name=dataset_name,\n",
    "    explanation_level=\"event\",\n",
    "    results_dir=str(RESOURCES_DIR / \"results\"),\n",
    "    debug_mode=False,\n",
    "    threshold_num=50,\n",
    "    save_results=True,\n",
    "    mcts_saved_dir=str(RESOURCES_DIR / \"results\" / \"tgnnexplainer_subgraphx\" / \"mcts_saved_dir\" ),\n",
    "    load_results=False,\n",
    "    rollout=30,\n",
    "    min_atoms=2,\n",
    "    c_puct=10.0,\n",
    "    use_navigator=False,            # set True + navigator_type (\"pg\"|\"mlp\"|\"dot\") if you use a navigator\n",
    "    # navigator_type=\"pg\",\n",
    "    # navigator_params={\"train_epochs\": 10, \"lr\": 1e-3, \"batch_size\": 64, \"explainer_ckpt_dir\": str(ROOT_DIR / \"xgraph\" / \"explainer_ckpts\")},\n",
    "    cache=True,\n",
    ")\n",
    "\n",
    "explainer = SubgraphXTGAdapter(adapter_cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 events to explain\n"
     ]
    }
   ],
   "source": [
    "# Load 1-based event indices you want to explain (same file the Hydra pipeline uses)\n",
    "target_event_idxs = load_explain_idx(explain_idx_csv, start=0)\n",
    "\n",
    "# Build anchors (we keep \"target_kind\" for compatibility; the crucial piece is event_idx)\n",
    "N = 5  # take a small batch first\n",
    "anchors = [{\"target_kind\": \"edge\", \"event_idx\": int(e)} for e in target_event_idxs[:N]]\n",
    "\n",
    "cfg = EvalConfig(\n",
    "    out_dir=\"runs\",\n",
    "    metrics={\n",
    "        \"sparsity\": {\n",
    "            \"eps\": 1e-6,\n",
    "            \"components\": [\"edges\", \"nodes\"]\n",
    "        },\n",
    "        # fidelity-minus at multiple K (drop top-k edges)\n",
    "        \"fidelity_minus\": {\n",
    "            \"topk\": [6, 12, 18],\n",
    "            \"result_as_logit\": True,     # set False if your model returns probabilities\n",
    "            \"normalize\": \"minmax\",       # for ranking importances\n",
    "            \"by\": \"value\"                # or \"abs\" if you prefer |w|\n",
    "        },\n",
    "        # (optional) sufficiency variant\n",
    "        \"fidelity_keep\": {\"topk\": [6, 12, 18], \"result_as_logit\": True}\n",
    "    },\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Runner config — 'sparsity' works out-of-the-box; (optional) add 'fidelity' once your model adapter supports masks\n",
    "runner = EvaluationRunner(\n",
    "    model=model,\n",
    "    dataset={\"events\": events, \"dataset_name\": dataset_name},\n",
    "    extractor=extractor,\n",
    "    explainers=[explainer],\n",
    "    config=cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "explain 0-th: 110314\n",
      "\n",
      "The nodes in graph is 146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mcts simulating: 100%|██████████| 30/30 [00:18<00:00,  1.66it/s, states=838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcts recorder saved at /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/results/candidate_scores/tgn_wikipedia_110314_mcts_recorder_pg_false_th50.csv\n",
      "results saved at /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/results/tgnnexplainer_subgraphx/mcts_saved_dir/tgn_wikipedia_110314_mcts_node_info_pg_false_th50.pt\n",
      "\n",
      "explain 1-th: 110832\n",
      "\n",
      "The nodes in graph is 488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mcts simulating: 100%|██████████| 30/30 [00:17<00:00,  1.70it/s, states=838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcts recorder saved at /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/results/candidate_scores/tgn_wikipedia_110832_mcts_recorder_pg_false_th50.csv\n",
      "results saved at /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/results/tgnnexplainer_subgraphx/mcts_saved_dir/tgn_wikipedia_110832_mcts_node_info_pg_false_th50.pt\n",
      "\n",
      "explain 2-th: 111397\n",
      "\n",
      "The nodes in graph is 447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mcts simulating: 100%|██████████| 30/30 [00:17<00:00,  1.73it/s, states=838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcts recorder saved at /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/results/candidate_scores/tgn_wikipedia_111397_mcts_recorder_pg_false_th50.csv\n",
      "results saved at /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/results/tgnnexplainer_subgraphx/mcts_saved_dir/tgn_wikipedia_111397_mcts_node_info_pg_false_th50.pt\n",
      "\n",
      "explain 3-th: 111915\n",
      "\n",
      "The nodes in graph is 732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mcts simulating: 100%|██████████| 30/30 [00:13<00:00,  2.20it/s, states=705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcts recorder saved at /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/results/candidate_scores/tgn_wikipedia_111915_mcts_recorder_pg_false_th50.csv\n",
      "results saved at /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/results/tgnnexplainer_subgraphx/mcts_saved_dir/tgn_wikipedia_111915_mcts_node_info_pg_false_th50.pt\n",
      "\n",
      "explain 4-th: 112473\n",
      "\n",
      "The nodes in graph is 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mcts simulating: 100%|██████████| 30/30 [00:14<00:00,  2.11it/s, states=705]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcts recorder saved at /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/results/candidate_scores/tgn_wikipedia_112473_mcts_recorder_pg_false_th50.csv\n",
      "results saved at /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/results/tgnnexplainer_subgraphx/mcts_saved_dir/tgn_wikipedia_112473_mcts_node_info_pg_false_th50.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'out_dir': 'runs/tgn_wikipedia_tgnn_explainer_subgraphx',\n",
       " 'jsonl': 'runs/tgn_wikipedia_tgnn_explainer_subgraphx/results.jsonl',\n",
       " 'csv': 'runs/tgn_wikipedia_tgnn_explainer_subgraphx/metrics.csv'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def _ensure_int_attr(obj, primary, alt_names, fallback):\n",
    "    val = getattr(obj, primary, None)\n",
    "    if val is None:\n",
    "        for n in alt_names:\n",
    "            v2 = getattr(obj, n, None)\n",
    "            if v2 is not None:\n",
    "                val = v2\n",
    "                break\n",
    "    if val is None:\n",
    "        val = fallback\n",
    "    try:\n",
    "        setattr(obj, primary, int(val))\n",
    "    except Exception:\n",
    "        setattr(obj, primary, fallback)\n",
    "\n",
    "# For layers: prefer .num_layers then .n_layers, else fallback k_hop (=2 default below)\n",
    "_ensure_int_attr(model, \"num_layers\", [\"n_layers\"], 2)\n",
    "\n",
    "# For neighbors: prefer .num_neighbors then .n_neighbors, else fallback (e.g., 20)\n",
    "_ensure_int_attr(model, \"num_neighbors\", [\"n_neighbors\"], 20)\n",
    "\n",
    "# Also make the runner pass explicit values (so extractor can use arguments if needed)\n",
    "out = runner.run(\n",
    "    anchors,\n",
    "    k_hop=getattr(model, \"num_layers\", 2) or 2,\n",
    "    num_neighbors=getattr(model, \"num_neighbors\", 20) or 20,\n",
    "    run_id=f\"{model_name}_{dataset_name}_tgnn_explainer_subgraphx\"\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>anchor_idx</th>\n",
       "      <th>explainer</th>\n",
       "      <th>elapsed_sec</th>\n",
       "      <th>sparsity.edges.n</th>\n",
       "      <th>sparsity.edges.l0</th>\n",
       "      <th>sparsity.edges.zero_frac</th>\n",
       "      <th>sparsity.edges.density</th>\n",
       "      <th>sparsity.edges.gini</th>\n",
       "      <th>sparsity.edges.entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>fidelity_drop.@12</th>\n",
       "      <th>fidelity_drop.prediction_drop.@18</th>\n",
       "      <th>fidelity_drop.@18</th>\n",
       "      <th>fidelity_keep.prediction_full</th>\n",
       "      <th>fidelity_keep.prediction_keep.@6</th>\n",
       "      <th>fidelity_keep.@6</th>\n",
       "      <th>fidelity_keep.prediction_keep.@12</th>\n",
       "      <th>fidelity_keep.@12</th>\n",
       "      <th>fidelity_keep.prediction_keep.@18</th>\n",
       "      <th>fidelity_keep.@18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tgn_wikipedia_tgnn_explainer_subgraphx</td>\n",
       "      <td>0</td>\n",
       "      <td>subgraphx_tg_tgn</td>\n",
       "      <td>22.358130</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011043</td>\n",
       "      <td>0.986843</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>0.983084</td>\n",
       "      <td>0.346144</td>\n",
       "      <td>0.636940</td>\n",
       "      <td>0.813248</td>\n",
       "      <td>0.169836</td>\n",
       "      <td>0.803670</td>\n",
       "      <td>0.179414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tgn_wikipedia_tgnn_explainer_subgraphx</td>\n",
       "      <td>1</td>\n",
       "      <td>subgraphx_tg_tgn</td>\n",
       "      <td>17.662668</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003990</td>\n",
       "      <td>0.997401</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.993409</td>\n",
       "      <td>0.371453</td>\n",
       "      <td>0.621956</td>\n",
       "      <td>0.226732</td>\n",
       "      <td>0.766677</td>\n",
       "      <td>0.195011</td>\n",
       "      <td>0.798398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tgn_wikipedia_tgnn_explainer_subgraphx</td>\n",
       "      <td>2</td>\n",
       "      <td>subgraphx_tg_tgn</td>\n",
       "      <td>17.354573</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.940</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.996200</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.996358</td>\n",
       "      <td>0.890276</td>\n",
       "      <td>0.106083</td>\n",
       "      <td>0.786143</td>\n",
       "      <td>0.210216</td>\n",
       "      <td>0.631153</td>\n",
       "      <td>0.365205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tgn_wikipedia_tgnn_explainer_subgraphx</td>\n",
       "      <td>3</td>\n",
       "      <td>subgraphx_tg_tgn</td>\n",
       "      <td>13.690124</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.991354</td>\n",
       "      <td>0.007672</td>\n",
       "      <td>0.999026</td>\n",
       "      <td>0.996413</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.948619</td>\n",
       "      <td>0.050407</td>\n",
       "      <td>0.989406</td>\n",
       "      <td>0.009620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tgn_wikipedia_tgnn_explainer_subgraphx</td>\n",
       "      <td>4</td>\n",
       "      <td>subgraphx_tg_tgn</td>\n",
       "      <td>14.275819</td>\n",
       "      <td>40</td>\n",
       "      <td>38</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.995579</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.997134</td>\n",
       "      <td>0.995140</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.992922</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.997057</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   run_id  anchor_idx         explainer  \\\n",
       "0  tgn_wikipedia_tgnn_explainer_subgraphx           0  subgraphx_tg_tgn   \n",
       "1  tgn_wikipedia_tgnn_explainer_subgraphx           1  subgraphx_tg_tgn   \n",
       "2  tgn_wikipedia_tgnn_explainer_subgraphx           2  subgraphx_tg_tgn   \n",
       "3  tgn_wikipedia_tgnn_explainer_subgraphx           3  subgraphx_tg_tgn   \n",
       "4  tgn_wikipedia_tgnn_explainer_subgraphx           4  subgraphx_tg_tgn   \n",
       "\n",
       "   elapsed_sec  sparsity.edges.n  sparsity.edges.l0  sparsity.edges.zero_frac  \\\n",
       "0    22.358130                50                 48                      0.96   \n",
       "1    17.662668                50                 48                      0.96   \n",
       "2    17.354573                50                 48                      0.96   \n",
       "3    13.690124                40                 38                      0.95   \n",
       "4    14.275819                40                 38                      0.95   \n",
       "\n",
       "   sparsity.edges.density  sparsity.edges.gini  sparsity.edges.entropy  ...  \\\n",
       "0                    0.04                0.940                0.693147  ...   \n",
       "1                    0.04                0.940                0.693147  ...   \n",
       "2                    0.04                0.940                0.693147  ...   \n",
       "3                    0.05                0.925                0.693147  ...   \n",
       "4                    0.05                0.925                0.693147  ...   \n",
       "\n",
       "   fidelity_drop.@12  fidelity_drop.prediction_drop.@18  fidelity_drop.@18  \\\n",
       "0           0.011043                           0.986843           0.003759   \n",
       "1           0.003990                           0.997401           0.003992   \n",
       "2           0.001030                           0.996200           0.000158   \n",
       "3           0.001230                           0.991354           0.007672   \n",
       "4           0.001276                           0.995579           0.001555   \n",
       "\n",
       "   fidelity_keep.prediction_full  fidelity_keep.prediction_keep.@6  \\\n",
       "0                       0.983084                          0.346144   \n",
       "1                       0.993409                          0.371453   \n",
       "2                       0.996358                          0.890276   \n",
       "3                       0.999026                          0.996413   \n",
       "4                       0.997134                          0.995140   \n",
       "\n",
       "   fidelity_keep.@6  fidelity_keep.prediction_keep.@12  fidelity_keep.@12  \\\n",
       "0          0.636940                           0.813248           0.169836   \n",
       "1          0.621956                           0.226732           0.766677   \n",
       "2          0.106083                           0.786143           0.210216   \n",
       "3          0.002613                           0.948619           0.050407   \n",
       "4          0.001994                           0.992922           0.004212   \n",
       "\n",
       "   fidelity_keep.prediction_keep.@18  fidelity_keep.@18  \n",
       "0                           0.803670           0.179414  \n",
       "1                           0.195011           0.798398  \n",
       "2                           0.631153           0.365205  \n",
       "3                           0.989406           0.009620  \n",
       "4                           0.997057           0.000077  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics (CSV): one row per (anchor, explainer)\n",
    "metrics_df = pd.read_csv(out[\"csv\"])\n",
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ea2e5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Fidelity monotonicity check failed for the following rows:\n  [fidelity_keep] row=0 (anchor=0, explainer=subgraphx_tg_tgn): ks=[6, 12, 18] values=[0.6369396716478828, 0.1698360988847915, 0.1794140770786234] :: Keeping more edges should not increase the drop in score.\n  [fidelity_keep] row=1 (anchor=1, explainer=subgraphx_tg_tgn): ks=[6, 12, 18] values=[0.6219558113796256, 0.7666771465682881, 0.7983978901267827] :: Keeping more edges should not increase the drop in score.\n  [fidelity_keep] row=2 (anchor=2, explainer=subgraphx_tg_tgn): ks=[6, 12, 18] values=[0.1060826576091397, 0.2102158365633641, 0.3652049407212189] :: Keeping more edges should not increase the drop in score.\n  [fidelity_keep] row=3 (anchor=3, explainer=subgraphx_tg_tgn): ks=[6, 12, 18] values=[0.0026126441836936, 0.0504068653314142, 0.0096203419281067] :: Keeping more edges should not increase the drop in score.\n  [fidelity_keep] row=4 (anchor=4, explainer=subgraphx_tg_tgn): ks=[6, 12, 18] values=[0.001994034958752, 0.0042117738600718, 7.688482328316315e-05] :: Keeping more edges should not increase the drop in score.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 122\u001b[0m\n\u001b[1;32m    118\u001b[0m         expl \u001b[38;5;241m=\u001b[39m metrics_df\u001b[38;5;241m.\u001b[39mat[row_idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplainer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplainer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m metrics_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         lines\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] row=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (anchor=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manchor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, explainer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): ks=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m values=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m :: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m         )\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lines))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric sanity summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m summary\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Fidelity monotonicity check failed for the following rows:\n  [fidelity_keep] row=0 (anchor=0, explainer=subgraphx_tg_tgn): ks=[6, 12, 18] values=[0.6369396716478828, 0.1698360988847915, 0.1794140770786234] :: Keeping more edges should not increase the drop in score.\n  [fidelity_keep] row=1 (anchor=1, explainer=subgraphx_tg_tgn): ks=[6, 12, 18] values=[0.6219558113796256, 0.7666771465682881, 0.7983978901267827] :: Keeping more edges should not increase the drop in score.\n  [fidelity_keep] row=2 (anchor=2, explainer=subgraphx_tg_tgn): ks=[6, 12, 18] values=[0.1060826576091397, 0.2102158365633641, 0.3652049407212189] :: Keeping more edges should not increase the drop in score.\n  [fidelity_keep] row=3 (anchor=3, explainer=subgraphx_tg_tgn): ks=[6, 12, 18] values=[0.0026126441836936, 0.0504068653314142, 0.0096203419281067] :: Keeping more edges should not increase the drop in score.\n  [fidelity_keep] row=4 (anchor=4, explainer=subgraphx_tg_tgn): ks=[6, 12, 18] values=[0.001994034958752, 0.0042117738600718, 7.688482328316315e-05] :: Keeping more edges should not increase the drop in score."
     ]
    }
   ],
   "source": [
    "# Basic sanity checks to make sure metric outputs look reasonable\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "if metrics_df.empty:\n",
    "    raise RuntimeError(\"metrics_df is empty; nothing to evaluate.\")\n",
    "\n",
    "fidelity_cols = [c for c in metrics_df.columns if c.startswith(\"fidelity_\")]\n",
    "sparsity_cols = [c for c in metrics_df.columns if \"sparsity\" in c]\n",
    "\n",
    "summary = {}\n",
    "\n",
    "# Fidelity checks\n",
    "fid_arrays = []\n",
    "for col in fidelity_cols:\n",
    "    vals = metrics_df[col].to_numpy(dtype=float)\n",
    "    finite = vals[np.isfinite(vals)]\n",
    "    if finite.size == 0:\n",
    "        raise RuntimeError(f\"{col} has no finite values (all NaN/inf).\")\n",
    "    fid_arrays.append(finite)\n",
    "    frac_nan = 1.0 - (finite.size / max(1, vals.size))\n",
    "    if frac_nan > 0:\n",
    "        print(f\"⚠️  {col}: {frac_nan:.1%} of values were NaN and were dropped from sanity checks.\")\n",
    "    neg_ratio = float((finite < 0).mean())\n",
    "    if neg_ratio > 0.5:\n",
    "        print(f\"⚠️  {col}: more than half of the finite values are negative ({neg_ratio:.1%}).\")\n",
    "\n",
    "if fid_arrays:\n",
    "    all_fid = np.concatenate(fid_arrays)\n",
    "    summary[\"fidelity\"] = {\n",
    "        \"mean\": float(all_fid.mean()),\n",
    "        \"std\": float(all_fid.std()),\n",
    "        \"min\": float(all_fid.min()),\n",
    "        \"max\": float(all_fid.max()),\n",
    "    }\n",
    "else:\n",
    "    print(\"No fidelity_* columns present; skipping fidelity sanity checks.\")\n",
    "\n",
    "# Sparsity checks (should live in [0, 1])\n",
    "ratio_suffixes = (\".zero_frac\", \".density\")\n",
    "count_suffixes = (\".n\", \".l0\")\n",
    "\n",
    "for col in sparsity_cols:\n",
    "    vals = metrics_df[col].to_numpy(dtype=float)\n",
    "    finite_mask = np.isfinite(vals)\n",
    "    finite = vals[finite_mask]\n",
    "    if finite.size == 0:\n",
    "        print(f\"⚠️  {col}: all values are NaN/inf; skipping.\")\n",
    "        continue\n",
    "    suffix = next((s for s in ratio_suffixes + count_suffixes if col.endswith(s)), None)\n",
    "    if suffix in count_suffixes:\n",
    "        if (finite < -1e-6).any():\n",
    "            raise RuntimeError(f\"{col} contains negative counts: min={finite.min():.3f}\")\n",
    "        summary.setdefault(\"sparsity_counts\", {})[col] = float(finite.mean())\n",
    "        continue\n",
    "    if suffix in ratio_suffixes:\n",
    "        if (finite < -1e-6).any() or (finite > 1 + 1e-6).any():\n",
    "            raise RuntimeError(\n",
    "                f\"{col} contains values outside [0, 1]: min={finite.min():.3f}, max={finite.max():.3f}\"\n",
    "            )\n",
    "        summary.setdefault(\"sparsity\", {})[col] = float(finite.mean())\n",
    "        continue\n",
    "    summary.setdefault(\"sparsity_misc\", {})[col] = float(finite.mean())\n",
    "    missing_frac = 1.0 - (finite.size / max(1, vals.size))\n",
    "    if missing_frac > 0:\n",
    "        print(f\"⚠️  {col}: skipped {missing_frac:.1%} missing values.\")\n",
    "\n",
    "# Monotonicity checks for fidelity@k (per row)\n",
    "trend_expectations = {\n",
    "    \"fidelity_minus\": (\"non_decreasing\", \"Dropping more edges should not reduce the drop in score.\"),\n",
    "    \"fidelity_keep\": (\"non_increasing\", \"Keeping more edges should not increase the drop in score.\"),\n",
    "}\n",
    "pattern = re.compile(r\"^(fidelity_(?:minus|keep))\\.@(\\d+)\")\n",
    "violations = []\n",
    "\n",
    "for prefix, (expect, desc) in trend_expectations.items():\n",
    "    prefix_cols = [c for c in fidelity_cols if c.startswith(prefix + \".@\")]\n",
    "    if not prefix_cols:\n",
    "        continue\n",
    "    # sort columns by the numeric k\n",
    "    col_info = []\n",
    "    for col in prefix_cols:\n",
    "        m = pattern.match(col)\n",
    "        if m:\n",
    "            col_info.append((int(m.group(2)), col))\n",
    "    col_info.sort()\n",
    "    if not col_info:\n",
    "        continue\n",
    "\n",
    "    for row_idx in metrics_df.index:\n",
    "        values = []\n",
    "        for k, col in col_info:\n",
    "            val = metrics_df.at[row_idx, col]\n",
    "            if val is None or (isinstance(val, float) and not np.isfinite(val)):\n",
    "                continue\n",
    "            val_f = float(val)\n",
    "            if not np.isfinite(val_f):\n",
    "                continue\n",
    "            values.append((k, val_f))\n",
    "        if len(values) < 2:\n",
    "            continue\n",
    "        values.sort()\n",
    "        ks = [k for k, _ in values]\n",
    "        seq = np.array([v for _, v in values], dtype=float)\n",
    "        diffs = np.diff(seq)\n",
    "        tol = 1e-6\n",
    "        if expect == \"non_decreasing\":\n",
    "            if np.any(diffs < -tol):\n",
    "                violations.append((prefix, row_idx, ks, seq.tolist(), desc))\n",
    "        else:  # non_increasing\n",
    "            if np.any(diffs > tol):\n",
    "                violations.append((prefix, row_idx, ks, seq.tolist(), desc))\n",
    "\n",
    "if violations:\n",
    "    lines = [\"Fidelity monotonicity check failed for the following rows:\"]\n",
    "    for prefix, row_idx, ks, seq, desc in violations[:5]:\n",
    "        anchor = metrics_df.at[row_idx, \"anchor_idx\"] if \"anchor_idx\" in metrics_df.columns else row_idx\n",
    "        expl = metrics_df.at[row_idx, \"explainer\"] if \"explainer\" in metrics_df.columns else \"?\"\n",
    "        lines.append(\n",
    "            f\"  [{prefix}] row={row_idx} (anchor={anchor}, explainer={expl}): ks={ks} values={seq} :: {desc}\"\n",
    "        )\n",
    "    raise RuntimeError(\"\\n\".join(lines))\n",
    "\n",
    "print(\"Metric sanity summary:\")\n",
    "for key, val in summary.items():\n",
    "    print(f\"  {key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num explanations: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(dict_keys(['context', 'result', 'metrics']),\n",
       " dict_keys(['explainer', 'elapsed_sec', 'importance_edges', 'importance_nodes', 'importance_time', 'extras']))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full JSONL (per-anchor payloads, including coalition and candidate list in \"extras\")\n",
    "import json\n",
    "with open(out[\"jsonl\"]) as f:\n",
    "    lines = [json.loads(line) for line in f]\n",
    "print(\"Num explanations:\", len(lines))\n",
    "lines[0].keys(), lines[0][\"result\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110314,\n",
       " [94457, 109720],\n",
       " [89019, 89030, 89679, 93888, 94342, 94345, 94351, 94353, 94386, 94450])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = lines[0]\n",
    "first[\"result\"][\"extras\"][\"event_idx\"], first[\"result\"][\"extras\"][\"coalition_eidx\"][:10], first[\"result\"][\"extras\"][\"candidate_eidx\"][:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
