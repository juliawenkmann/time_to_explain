{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4aa224",
   "metadata": {},
   "source": [
    "# Train A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f528aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook config: seed=42, device=mps\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _bootstrap_repo_root(start: Path | None = None) -> Path:\n",
    "    here = (start or Path.cwd()).resolve()\n",
    "    for candidate in (here, *here.parents):\n",
    "        if (candidate / \"time_to_explain\").is_dir():\n",
    "            return candidate\n",
    "    raise RuntimeError(\n",
    "        f\"Could not locate the repository root from {here}. \"\n",
    "        \"Set PROJECT_ROOT manually if your layout is unusual.\"\n",
    "    )\n",
    "\n",
    "\n",
    "PROJECT_ROOT = _bootstrap_repo_root()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from time_to_explain.utils.device import pick_device\n",
    "CONFIG_PATH = PROJECT_ROOT / \"configs\" / \"notebooks\" / \"global.json\"\n",
    "NOTEBOOK_CFG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\")) if CONFIG_PATH.exists() else {}\n",
    "SEED = int(NOTEBOOK_CFG.get(\"seed\", 42))\n",
    "DEVICE = pick_device(NOTEBOOK_CFG.get(\"device\", \"auto\"))\n",
    "print(f\"Notebook config: seed={SEED}, device={DEVICE}\")\n",
    "\n",
    "from time_to_explain.models.utils import (\n",
    "    build_cmd,\n",
    "    ensure_tempme_processed,\n",
    "    ensure_workdir,\n",
    "    export_trained_models,\n",
    "    prepare_env,\n",
    "    run_cmd,\n",
    ")\n",
    "from time_to_explain.utils.cli import (\n",
    "    args_dict_to_list,\n",
    "    normalize_datasets,\n",
    "    resolve_path,\n",
    "    slugify,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8b691",
   "metadata": {},
   "source": [
    "### Set Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e8c8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"wikipedia\"  # e.g. \"reddit\", \"simulate_v1\", ...\n",
    "# Choose one of: \"tgn\", \"tgat\", \"graphmixer\", \"dbgnn\"\n",
    "MODEL_TYPE = \"tgn\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a6b4e",
   "metadata": {},
   "source": [
    "### Get Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e29aa4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain\n",
      "MODEL_TYPE: TGN\n",
      "DATASETS: ['wikipedia']\n",
      "Artifacts root: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/models/runs\n",
      "Sample run directory: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/models/runs/tgn_wikipedia\n",
      "Configuration loaded from: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/configs/models/train_tgn_wikipedia.json\n"
     ]
    }
   ],
   "source": [
    "CONFIG_PATH = Path(f\"configs/models/train_{MODEL_TYPE}_{DATASET_NAME}.json\")\n",
    "\n",
    "CONFIG_PATH = resolve_path(str(CONFIG_PATH), root=PROJECT_ROOT)\n",
    "\n",
    "if CONFIG_PATH is None or not CONFIG_PATH.exists():\n",
    "    print(f\"[warn] Config not found: {CONFIG_PATH}. Using notebook defaults.\")\n",
    "    CONFIG = {}\n",
    "else:\n",
    "    CONFIG = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "# Prefer config override, otherwise keep the notebook selection above.\n",
    "MODEL_TYPE = str(CONFIG.get(\"model_type\", MODEL_TYPE)).upper()\n",
    "DATASET_LIST = normalize_datasets(CONFIG.get(\"datasets\", [DATASET_NAME]))\n",
    "PYTHON_BIN = str(CONFIG.get(\"python_bin\", \"python\"))\n",
    "DRY_RUN = bool(CONFIG.get(\"dry_run\", False))\n",
    "CUDA_VISIBLE_DEVICES = CONFIG.get(\"cuda_visible_devices\")\n",
    "\n",
    "MODEL_SPECS = {str(k).upper(): v for k, v in (CONFIG.get(\"models\") or {}).items()}\n",
    "\n",
    "# Only enforce model-spec existence for the script-driven baselines.\n",
    "if MODEL_TYPE in {\"TGN\", \"TGAT\", \"GRAPHMIXER\"} and MODEL_TYPE not in MODEL_SPECS:\n",
    "    raise KeyError(\n",
    "        f\"Model spec for {MODEL_TYPE} missing in {CONFIG_PATH}. \"\n",
    "        f\"Add it under CONFIG['models'] or switch MODEL_TYPE.\"\n",
    "    )\n",
    "\n",
    "RESOURCES_MODELS = resolve_path(CONFIG.get(\"resources_models_dir\", \"resources/models\"), root=PROJECT_ROOT)\n",
    "RUNS_ROOT = resolve_path(CONFIG.get(\"runs_root\", \"resources/models/runs\"), root=PROJECT_ROOT)\n",
    "RESOURCES_DATASETS = resolve_path(CONFIG.get(\"resources_datasets_dir\", \"resources/datasets/processed\"), root=PROJECT_ROOT)\n",
    "\n",
    "DEFAULT_WORKDIR = RUNS_ROOT / f\"{slugify(MODEL_TYPE)}_{slugify(DATASET_LIST[0])}\"\n",
    "\n",
    "TGN_SPEC = MODEL_SPECS.get(\"TGN\", {})\n",
    "TGAT_SPEC = MODEL_SPECS.get(\"TGAT\", {})\n",
    "GRAPHMIXER_SPEC = MODEL_SPECS.get(\"GRAPHMIXER\", {})\n",
    "DBGNN_SPEC = MODEL_SPECS.get(\"DBGNN\", {})  # optional (not required for in-notebook trainer)\n",
    "\n",
    "def _maybe_resolve(path_like):\n",
    "    return resolve_path(path_like, root=PROJECT_ROOT) if path_like else None\n",
    "\n",
    "TGN_SCRIPT = _maybe_resolve(TGN_SPEC.get(\"script\"))\n",
    "TGAT_SCRIPT = _maybe_resolve(TGAT_SPEC.get(\"script\"))\n",
    "GRAPHMIXER_SCRIPT = _maybe_resolve(GRAPHMIXER_SPEC.get(\"script\"))\n",
    "GRAPHMIXER_PROCESSED_DIR = _maybe_resolve(GRAPHMIXER_SPEC.get(\"processed_dir\"))\n",
    "GRAPHMIXER_PARAMS_DIR = _maybe_resolve(GRAPHMIXER_SPEC.get(\"params_dir\"))\n",
    "\n",
    "DBGNN_SCRIPT = _maybe_resolve(DBGNN_SPEC.get(\"script\"))  # optional\n",
    "\n",
    "def get_tgn_args(dataset: str) -> list[str]:\n",
    "    return args_dict_to_list(TGN_SPEC.get(\"args\", {}), dataset)\n",
    "\n",
    "def get_tgat_args(dataset: str) -> list[str]:\n",
    "    return args_dict_to_list(TGAT_SPEC.get(\"args\", {}), dataset)\n",
    "\n",
    "def get_graphmixer_args(dataset: str) -> list[str]:\n",
    "    return args_dict_to_list(GRAPHMIXER_SPEC.get(\"args\", {}), dataset)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"MODEL_TYPE:\", MODEL_TYPE)\n",
    "print(\"DATASETS:\", DATASET_LIST)\n",
    "print(\"Artifacts root:\", RUNS_ROOT)\n",
    "print(\"Sample run directory:\", DEFAULT_WORKDIR)\n",
    "if CONFIG:\n",
    "    print(\"Configuration loaded from:\", CONFIG_PATH)\n",
    "else:\n",
    "    print(\"No config loaded (using notebook defaults).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3adc194",
   "metadata": {},
   "source": [
    "### DBGNN: train on Wikipedia (and other TGNN-style datasets)\n",
    "\n",
    "This section adds an **in-notebook** trainer for the DBGNN model used in `01_train_dbgnn_and_node2vec.ipynb`.\n",
    "\n",
    "It uses the TGNN processed files in `resources/datasets/processed/` (e.g. `ml_wikipedia.csv`, `ml_wikipedia_node.npy`).\n",
    "By default it trains a **node classification** task on Wikipedia where the label is the node role:\n",
    "- `0` = appears as a source node `u` (user)\n",
    "- `1` = appears as a destination node `i` (page)\n",
    "\n",
    "You can change the task/labels later if you want (see `DBGNN_DEFAULTS`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c0138ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBGNN utilities ready.\n"
     ]
    }
   ],
   "source": [
    "# --- DBGNN utilities (used when MODEL_TYPE=\"DBGNN\") ---\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Make gnnbench (src/) importable (same trick as in 01_train_dbgnn_and_node2vec.ipynb).\n",
    "SRC_ROOT = PROJECT_ROOT / \"src\"\n",
    "if SRC_ROOT.exists() and str(SRC_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_ROOT))\n",
    "\n",
    "# Optional dependency (used only if you toggle use_node2vec_features=True)\n",
    "_HAS_NODE2VEC = True\n",
    "try:\n",
    "    from node2vec import Node2Vec  # noqa: F401\n",
    "except Exception:\n",
    "    _HAS_NODE2VEC = False\n",
    "\n",
    "# torch_geometric is used for the Data container. We fall back to a lightweight object if missing.\n",
    "try:\n",
    "    from torch_geometric.data import Data  # type: ignore\n",
    "except Exception:\n",
    "    Data = None\n",
    "\n",
    "DBGNN_DEFAULTS: dict[str, Any] = dict(\n",
    "    # Task definition ---------------------------------------------------\n",
    "    task=\"node_type\",  # \"node_type\" (default) | \"link_pred\" (experimental stub)\n",
    "    # Train/val/test split ---------------------------------------------\n",
    "    num_test=0.30,      # fraction of labeled nodes\n",
    "    num_val=0.10,       # fraction of labeled nodes (taken from remaining after test split)\n",
    "    seed=SEED,\n",
    "    # Model/training hyperparams ---------------------------------------\n",
    "    epochs=400,\n",
    "    lr=1e-3,\n",
    "    p_dropout=0.4,\n",
    "    hidden_dims=(16, 32, 16),\n",
    "    # Features ----------------------------------------------------------\n",
    "    use_node2vec_features=False,\n",
    "    # If None, will be set to hidden_dims[0] (keeps dimensionalities consistent).\n",
    "    n2v_dim=None,\n",
    "    # Higher-order graph construction ----------------------------------\n",
    "    # \"identity\" means: HO graph == FO graph, HO nodes == FO nodes (fast and robust).\n",
    "    # You can replace this with a real HO construction later.\n",
    "    ho_mode=\"identity\",\n",
    ")\n",
    "\n",
    "def _read_tgnn_processed(dataset: str, processed_dir: Path):\n",
    "    \"\"\"Read TGNN-style processed files: ml_{dataset}.csv and ml_{dataset}_node.npy.\"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    csv_path = processed_dir / f\"ml_{dataset}.csv\"\n",
    "    node_path = processed_dir / f\"ml_{dataset}_node.npy\"\n",
    "\n",
    "    # If processed files are missing, try to generate them using the repo helper.\n",
    "    if not csv_path.exists() or not node_path.exists():\n",
    "        try:\n",
    "            from time_to_explain.data.tgnn_setup import setup_tgnn_data\n",
    "            print(f\"[DBGNN] Processed files missing for '{dataset}'. Running setup_tgnn_data(...)\")\n",
    "            setup_tgnn_data(root=PROJECT_ROOT, only=[dataset], force=False, do_process=True)\n",
    "        except Exception as e:\n",
    "            print(f\"[DBGNN] Auto-processing failed: {e}\")\n",
    "\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing TGNN CSV: {csv_path}\")\n",
    "    if not node_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing TGNN node features: {node_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Standard TGNN/TGN column names are: u, i, ts, label, idx\n",
    "    # If your CSV has no header, try to repair it.\n",
    "    expected = {\"u\", \"i\", \"ts\"}\n",
    "    if not expected.issubset(set(df.columns)):\n",
    "        # Common case: unnamed columns 0..4\n",
    "        if set(df.columns) >= {0, 1, 2}:\n",
    "            df = df.rename(columns={0: \"u\", 1: \"i\", 2: \"ts\"})\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected columns in {csv_path}: {list(df.columns)}\")\n",
    "\n",
    "    node_feat = np.load(node_path)\n",
    "    return df, node_feat\n",
    "\n",
    "def _infer_num_nodes(df, node_feat: np.ndarray) -> int:\n",
    "    max_id = int(max(df[\"u\"].max(), df[\"i\"].max()))\n",
    "    # node_feat might be larger than max_id+1 (some preprocessors include unused ids)\n",
    "    return int(max(max_id + 1, node_feat.shape[0]))\n",
    "\n",
    "def _make_node_type_labels(df, num_nodes: int) -> np.ndarray:\n",
    "    \"\"\"0 for nodes that appear in df['u'], 1 for nodes that appear in df['i'], -1 otherwise.\"\"\"\n",
    "    y = -np.ones(int(num_nodes), dtype=np.int64)\n",
    "    u_nodes = set(int(x) for x in df[\"u\"].to_numpy())\n",
    "    i_nodes = set(int(x) for x in df[\"i\"].to_numpy())\n",
    "\n",
    "    # default bipartite assumption: u != i\n",
    "    for n in u_nodes:\n",
    "        if 0 <= n < num_nodes:\n",
    "            y[n] = 0\n",
    "    for n in i_nodes:\n",
    "        if 0 <= n < num_nodes:\n",
    "            if y[n] == 0:\n",
    "                # If overlap happens (rare), mark as a 3rd class.\n",
    "                y[n] = 2\n",
    "            else:\n",
    "                y[n] = 1\n",
    "    return y\n",
    "\n",
    "def _make_stratified_masks(y: np.ndarray, *, num_test: float, num_val: float, seed: int):\n",
    "    \"\"\"Create boolean train/val/test masks for labeled nodes (y>=0).\"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    y = np.asarray(y)\n",
    "    idx = np.arange(len(y))\n",
    "    labeled = (y >= 0)\n",
    "    idx_lab = idx[labeled]\n",
    "    y_lab = y[labeled]\n",
    "\n",
    "    if idx_lab.size == 0:\n",
    "        raise ValueError(\"No labeled nodes found (y < 0 everywhere).\")\n",
    "\n",
    "    idx_trainval, idx_test = train_test_split(\n",
    "        idx_lab, test_size=float(num_test), random_state=int(seed), stratify=y_lab\n",
    "    )\n",
    "    # val fraction is defined w.r.t. labeled nodes, so convert to fraction of remaining\n",
    "    val_frac_of_trainval = float(num_val) / float(1.0 - num_test)\n",
    "    idx_train, idx_val = train_test_split(\n",
    "        idx_trainval, test_size=val_frac_of_trainval, random_state=int(seed), stratify=y[idx_trainval]\n",
    "    )\n",
    "\n",
    "    train_mask = np.zeros(len(y), dtype=bool); train_mask[idx_train] = True\n",
    "    val_mask = np.zeros(len(y), dtype=bool);   val_mask[idx_val] = True\n",
    "    test_mask = np.zeros(len(y), dtype=bool);  test_mask[idx_test] = True\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "def _build_fo_graph(df, *, num_nodes: int, undirected: bool = True, make_unique: bool = True):\n",
    "    u = df[\"u\"].to_numpy(dtype=np.int64)\n",
    "    v = df[\"i\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "    if undirected:\n",
    "        src = np.concatenate([u, v], axis=0)\n",
    "        dst = np.concatenate([v, u], axis=0)\n",
    "    else:\n",
    "        src, dst = u, v\n",
    "\n",
    "    edge_index = np.stack([src, dst], axis=0)\n",
    "\n",
    "    if make_unique:\n",
    "        # Unique edges to reduce duplicates (Wikipedia has many repeated interactions)\n",
    "        # Convert to structured array for fast unique.\n",
    "        e = edge_index.T\n",
    "        e = np.unique(e, axis=0)\n",
    "        edge_index = e.T\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    edge_weight = torch.ones(edge_index.size(1), dtype=torch.float32)\n",
    "    return edge_index, edge_weight\n",
    "\n",
    "def _build_identity_higher_order(\n",
    "    *, num_nodes: int, x: torch.Tensor, edge_index: torch.Tensor, edge_weight: torch.Tensor\n",
    "):\n",
    "    \"\"\"HO graph == FO graph; HO nodes == FO nodes; bipartite edges are identity.\"\"\"\n",
    "    x_h = x.clone()\n",
    "    edge_index_ho = edge_index.clone()\n",
    "    edge_weight_ho = edge_weight.clone()\n",
    "    ids = torch.arange(int(num_nodes), dtype=torch.long)\n",
    "    bipartite_edge_index = torch.stack([ids, ids], dim=0)  # [2, N]: [ho_id, fo_id]\n",
    "    return x_h, edge_index_ho, edge_weight_ho, bipartite_edge_index\n",
    "\n",
    "def _maybe_apply_node2vec_features(data, *, dim: int, seed: int):\n",
    "    if not _HAS_NODE2VEC:\n",
    "        raise RuntimeError(\n",
    "            \"node2vec is not installed. Install it (pip install node2vec) or set use_node2vec_features=False.\"\n",
    "        )\n",
    "    import networkx as nx\n",
    "\n",
    "    # Build a NetworkX graph from FO edges\n",
    "    ei = data.edge_index.detach().cpu().numpy()\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(ei.T.tolist())\n",
    "\n",
    "    # Learn embeddings (FO). We reuse the same for HO if ho_mode == identity.\n",
    "    n2v = Node2Vec(\n",
    "        G,\n",
    "        dimensions=int(dim),\n",
    "        walk_length=30,\n",
    "        num_walks=10,\n",
    "        p=1.0,\n",
    "        q=1.0,\n",
    "        workers=1,\n",
    "        seed=int(seed),\n",
    "    )\n",
    "    w2v = n2v.fit(window=10, min_count=1, batch_words=128)\n",
    "\n",
    "    emb = np.zeros((int(data.num_nodes), int(dim)), dtype=np.float32)\n",
    "    for n in range(int(data.num_nodes)):\n",
    "        emb[n] = w2v.wv[str(n)]\n",
    "    x_new = torch.tensor(emb, device=data.x.device, dtype=torch.float32)\n",
    "\n",
    "    data.x = x_new\n",
    "    # If HO nodes == FO nodes, we can reuse embeddings.\n",
    "    if hasattr(data, \"x_h\") and data.x_h is not None and data.x_h.size(0) == data.x.size(0):\n",
    "        data.x_h = x_new.clone()\n",
    "\n",
    "    return data\n",
    "\n",
    "# --- Metrics helpers (copied from 01_train_dbgnn_and_node2vec.ipynb) ---\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def _predict_labels(model, data):\n",
    "    model.eval()\n",
    "    logits = model(data)\n",
    "    return logits.argmax(dim=1)\n",
    "\n",
    "def _macro_scores(y_true: np.ndarray, y_pred: np.ndarray) -> dict[str, float]:\n",
    "    labels = np.unique(y_true)\n",
    "    return dict(\n",
    "        accuracy=float(accuracy_score(y_true, y_pred)),\n",
    "        precision_macro=float(precision_score(y_true, y_pred, average=\"macro\", labels=labels, zero_division=0)),\n",
    "        recall_macro=float(recall_score(y_true, y_pred, average=\"macro\", labels=labels, zero_division=0)),\n",
    "        f1_macro=float(f1_score(y_true, y_pred, average=\"macro\", labels=labels, zero_division=0)),\n",
    "    )\n",
    "\n",
    "def _evaluate_macro_metrics(model, data):\n",
    "    y = data.y.detach().cpu().numpy()\n",
    "    pred = _predict_labels(model, data).detach().cpu().numpy()\n",
    "\n",
    "    train_mask = getattr(data, \"train_mask\", None)\n",
    "    test_mask = getattr(data, \"test_mask\", None)\n",
    "    if train_mask is None or test_mask is None:\n",
    "        raise ValueError(\"data.train_mask / data.test_mask are required for evaluation\")\n",
    "\n",
    "    train_mask = train_mask.detach().cpu().numpy().astype(bool)\n",
    "    test_mask = test_mask.detach().cpu().numpy().astype(bool)\n",
    "\n",
    "    labeled = (y >= 0)\n",
    "    train_idx = train_mask & labeled\n",
    "    test_idx = test_mask & labeled\n",
    "\n",
    "    train_metrics = _macro_scores(y[train_idx], pred[train_idx])\n",
    "    test_metrics = _macro_scores(y[test_idx], pred[test_idx])\n",
    "    return train_metrics, test_metrics\n",
    "\n",
    "def _evaluate_balanced_accuracy(model, data):\n",
    "    train_metrics, test_metrics = _evaluate_macro_metrics(model, data)\n",
    "    # Balanced accuracy == macro recall for multi-class\n",
    "    return train_metrics[\"recall_macro\"], test_metrics[\"recall_macro\"]\n",
    "\n",
    "def _build_dbgnn_model(data, *, device, hidden_dims, p_dropout):\n",
    "    from pathpyG.nn.dbgnn import DBGNN\n",
    "\n",
    "    # Use only labeled nodes to determine number of classes.\n",
    "    y_lab = data.y[data.y >= 0]\n",
    "    num_classes = int(y_lab.unique().numel()) if y_lab.numel() > 0 else int(data.y.unique().numel())\n",
    "\n",
    "    num_features = (int(data.x.size(1)), int(data.x_h.size(1)))\n",
    "    model = DBGNN(\n",
    "        num_features=num_features,\n",
    "        num_classes=int(num_classes),\n",
    "        hidden_dims=list(hidden_dims),\n",
    "        p_dropout=float(p_dropout),\n",
    "    ).to(device)\n",
    "    return model\n",
    "\n",
    "def train_dbgnn_node_type(\n",
    "    dataset: str,\n",
    "    *,\n",
    "    workdir: Path,\n",
    "    processed_dir: Path,\n",
    "    device: torch.device,\n",
    "    cfg: dict[str, Any] | None = None,\n",
    "):\n",
    "    \"\"\"Train DBGNN on Wikipedia (or another TGNN-style dataset) using node-type labels.\"\"\"\n",
    "    cfg = dict(DBGNN_DEFAULTS if cfg is None else cfg)\n",
    "\n",
    "    df, node_feat_np = _read_tgnn_processed(dataset, processed_dir)\n",
    "    num_nodes = _infer_num_nodes(df, node_feat_np)\n",
    "\n",
    "    # Build labels + masks\n",
    "    y_np = _make_node_type_labels(df, num_nodes)\n",
    "    train_mask_np, val_mask_np, test_mask_np = _make_stratified_masks(\n",
    "        y_np, num_test=float(cfg[\"num_test\"]), num_val=float(cfg[\"num_val\"]), seed=int(cfg[\"seed\"])\n",
    "    )\n",
    "\n",
    "    # Features (TGNN provides node features)\n",
    "    x = torch.tensor(node_feat_np[:num_nodes], dtype=torch.float32)\n",
    "\n",
    "    # FO graph from interactions (static, deduplicated)\n",
    "    edge_index, edge_weight = _build_fo_graph(df, num_nodes=num_nodes, undirected=True, make_unique=True)\n",
    "\n",
    "    # HO graph (default: identity)\n",
    "    ho_mode = str(cfg.get(\"ho_mode\", \"identity\")).lower()\n",
    "    if ho_mode != \"identity\":\n",
    "        print(f\"[DBGNN] ho_mode='{ho_mode}' not implemented in this notebook. Falling back to 'identity'.\")\n",
    "        ho_mode = \"identity\"\n",
    "\n",
    "    x_h, edge_index_ho, edge_weight_ho, bip_ei = _build_identity_higher_order(\n",
    "        num_nodes=num_nodes, x=x, edge_index=edge_index, edge_weight=edge_weight\n",
    "    )\n",
    "\n",
    "    # Assemble data object\n",
    "    y = torch.tensor(y_np, dtype=torch.long)\n",
    "    train_mask = torch.tensor(train_mask_np, dtype=torch.bool)\n",
    "    val_mask = torch.tensor(val_mask_np, dtype=torch.bool)\n",
    "    test_mask = torch.tensor(test_mask_np, dtype=torch.bool)\n",
    "\n",
    "    if Data is not None:\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            edge_index=edge_index,\n",
    "        )\n",
    "        # Keep both common spellings for edge weights for maximum compatibility.\n",
    "        data.edge_weights = edge_weight\n",
    "        data.edge_weight = edge_weight\n",
    "\n",
    "        # Extra attrs expected by DBGNN experiments\n",
    "        data.x_h = x_h\n",
    "        data.edge_index_higher_order = edge_index_ho\n",
    "        data.edge_weights_higher_order = edge_weight_ho\n",
    "        data.bipartite_edge_index = bip_ei\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data.test_mask = test_mask\n",
    "        data.num_nodes = int(num_nodes)\n",
    "    else:\n",
    "        from types import SimpleNamespace\n",
    "\n",
    "        data = SimpleNamespace(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            edge_index=edge_index,\n",
    "            edge_weights=edge_weight,\n",
    "            edge_weight=edge_weight,\n",
    "            x_h=x_h,\n",
    "            edge_index_higher_order=edge_index_ho,\n",
    "            edge_weights_higher_order=edge_weight_ho,\n",
    "            bipartite_edge_index=bip_ei,\n",
    "            train_mask=train_mask,\n",
    "            val_mask=val_mask,\n",
    "            test_mask=test_mask,\n",
    "            num_nodes=int(num_nodes),\n",
    "        )\n",
    "\n",
    "    # Move tensors to device\n",
    "    def _to_dev(v):\n",
    "        return v.to(device) if torch.is_tensor(v) else v\n",
    "\n",
    "    for attr in [\n",
    "        \"x\",\n",
    "        \"y\",\n",
    "        \"edge_index\",\n",
    "        \"edge_weight\",\n",
    "        \"x_h\",\n",
    "        \"edge_index_higher_order\",\n",
    "        \"edge_weights_higher_order\",\n",
    "        \"bipartite_edge_index\",\n",
    "        \"train_mask\",\n",
    "        \"val_mask\",\n",
    "        \"test_mask\",\n",
    "    ]:\n",
    "        if hasattr(data, attr):\n",
    "            setattr(data, attr, _to_dev(getattr(data, attr)))\n",
    "\n",
    "    # Optional: overwrite features with Node2Vec embeddings\n",
    "    if bool(cfg.get(\"use_node2vec_features\", False)):\n",
    "        dim = cfg.get(\"n2v_dim\") or int(cfg[\"hidden_dims\"][0])\n",
    "        data = _maybe_apply_node2vec_features(data, dim=int(dim), seed=int(cfg[\"seed\"]))\n",
    "\n",
    "    # Build model\n",
    "    model = _build_dbgnn_model(\n",
    "        data, device=device, hidden_dims=cfg[\"hidden_dims\"], p_dropout=cfg[\"p_dropout\"]\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=float(cfg[\"lr\"]))\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    losses = []\n",
    "    train_ba_hist = []\n",
    "    test_ba_hist = []\n",
    "\n",
    "    for epoch in range(int(cfg[\"epochs\"])):\n",
    "        model.train()\n",
    "        logits = model(data)\n",
    "        loss = loss_fn(logits[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        losses.append(float(loss.detach().cpu().item()))\n",
    "        train_ba, test_ba = _evaluate_balanced_accuracy(model, data)\n",
    "        train_ba_hist.append(float(train_ba))\n",
    "        test_ba_hist.append(float(test_ba))\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(\n",
    "                f\"[DBGNN:{dataset}] epoch={epoch:04d}  loss={loss.item():.4f}  \"\n",
    "                f\"train_bal_acc={train_ba:.4f}  test_bal_acc={test_ba:.4f}\"\n",
    "            )\n",
    "\n",
    "    # Save artifacts\n",
    "    workdir = Path(workdir)\n",
    "    workdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ckpt_path = workdir / \"dbgnn_node_type.pt\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"cfg\": cfg,\n",
    "            \"dataset\": dataset,\n",
    "        },\n",
    "        ckpt_path,\n",
    "    )\n",
    "\n",
    "    metrics_path = workdir / \"dbgnn_train_metrics.json\"\n",
    "    train_metrics, test_metrics = _evaluate_macro_metrics(model, data)\n",
    "    import json as _json\n",
    "\n",
    "    metrics_path.write_text(\n",
    "        _json.dumps(\n",
    "            {\n",
    "                \"train\": train_metrics,\n",
    "                \"test\": test_metrics,\n",
    "                \"losses\": losses,\n",
    "                \"train_bal_acc\": train_ba_hist,\n",
    "                \"test_bal_acc\": test_ba_hist,\n",
    "            },\n",
    "            indent=2,\n",
    "        ),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    print(f\"[DBGNN:{dataset}] Saved checkpoint -> {ckpt_path}\")\n",
    "    print(f\"[DBGNN:{dataset}] Saved metrics    -> {metrics_path}\")\n",
    "\n",
    "    # Also export under resources/models/<dataset>/dbgnn for consistency with other baselines.\n",
    "    export_dir = (RESOURCES_MODELS / slugify(dataset) / \"dbgnn\")\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "    exported_ckpt = export_dir / ckpt_path.name\n",
    "    exported_ckpt.write_bytes(ckpt_path.read_bytes())\n",
    "    print(f\"[DBGNN:{dataset}] Exported checkpoint -> {exported_ckpt}\")\n",
    "\n",
    "    return model, data, {\n",
    "        \"ckpt\": str(ckpt_path),\n",
    "        \"metrics\": str(metrics_path),\n",
    "        \"exported_ckpt\": str(exported_ckpt),\n",
    "    }\n",
    "\n",
    "print(\"DBGNN utilities ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea57cf91",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad1145db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Launching TGN on wikipedia ===\n",
      "Artifacts will be stored under: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/models/runs/tgn_wikipedia\n",
      "$ (cwd=/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/models/runs/tgn_wikipedia) python /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/models/tgn/train_self_supervised.py --data wikipedia --use_memory --prefix tgn-attn --n_runs 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Namespace(data='wikipedia', bs=200, prefix='tgn-attn', n_degree=10, n_head=2, n_epoch=50, n_layer=1, lr=0.0001, patience=5, n_runs=10, drop_out=0.1, gpu=0, node_dim=100, time_dim=100, backprop_every=1, use_memory=True, embedding_module='graph_attention', message_function='identity', memory_updater='gru', aggregator='last', memory_update_at_end=False, message_dim=100, memory_dim=172, different_new_nodes=False, uniform=False, randomize_features=False, use_destination_embedding_in_message=False, use_source_embedding_in_message=False, dyrep=False)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/models/tgn/train_self_supervised.py\", line 181, in <module>\n",
      "    ) = get_data(\n",
      "        ^^^^^^^^^\n",
      "  File \"/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/models/tgn/utils/data_processing.py\", line 100, in get_data\n",
      "    graph_df = pd.read_csv(graph_df_file)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/juliawenkmann/miniconda3/envs/ml/lib/python3.11/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/juliawenkmann/miniconda3/envs/ml/lib/python3.11/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/juliawenkmann/miniconda3/envs/ml/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/juliawenkmann/miniconda3/envs/ml/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/juliawenkmann/miniconda3/envs/ml/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/juliawenkmann/miniconda3/envs/ml/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1735, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"/Users/juliawenkmann/miniconda3/envs/ml/lib/python3.11/site-packages/pandas/io/common.py\", line 856, in get_handle\n",
      "    handle = open(\n",
      "             ^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/datasets/processed/wikipedia/ml_wikipedia.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain\n",
      "[ERROR] process exited with code 1\n",
      "[TGN] Training failed for wikipedia - see output above.\n"
     ]
    }
   ],
   "source": [
    "# --- Launch training for each dataset ---\n",
    "env = prepare_env(project_root=PROJECT_ROOT, cuda_visible_devices=CUDA_VISIBLE_DEVICES)\n",
    "\n",
    "for dataset in DATASET_LIST:\n",
    "    workdir = ensure_workdir(RUNS_ROOT, MODEL_TYPE, dataset)\n",
    "    print(\"=== Launching\", MODEL_TYPE.upper(), \"on\", dataset, \"===\")\n",
    "    print(\"Artifacts will be stored under:\", workdir)\n",
    "\n",
    "    if MODEL_TYPE.upper() == \"TGN\":\n",
    "        cmd = build_cmd(PYTHON_BIN, TGN_SCRIPT, get_tgn_args(dataset))\n",
    "        code = run_cmd(cmd, env=env, workdir=workdir, dry_run=DRY_RUN)\n",
    "        if code == 0:\n",
    "            export_trained_models(MODEL_TYPE, dataset, workdir, RESOURCES_MODELS)\n",
    "            print(\"[TGN] Training completed (or started successfully) for\", dataset)\n",
    "        else:\n",
    "            print(\"[TGN] Training failed for\", dataset, \"- see output above.\")\n",
    "\n",
    "    elif MODEL_TYPE.upper() == \"TGAT\":\n",
    "        try:\n",
    "            cmd = build_cmd(PYTHON_BIN, TGAT_SCRIPT, get_tgat_args(dataset))\n",
    "        except FileNotFoundError as exc:\n",
    "            raise FileNotFoundError(\n",
    "                f\"{exc} Update configs/models to point to your TGAT training script.\"\n",
    "            )\n",
    "        code = run_cmd(cmd, env=env, workdir=workdir, dry_run=DRY_RUN)\n",
    "        if code == 0:\n",
    "            export_trained_models(MODEL_TYPE, dataset, workdir, RESOURCES_MODELS)\n",
    "            print(\"[TGAT] Training completed (or started successfully) for\", dataset)\n",
    "        else:\n",
    "            print(\"[TGAT] Training failed for\", dataset, \"- see output above.\")\n",
    "\n",
    "    elif MODEL_TYPE.upper() == \"GRAPHMIXER\":\n",
    "        if GRAPHMIXER_PROCESSED_DIR is None:\n",
    "            raise ValueError(\"GRAPHMIXER processed_dir missing in config.\")\n",
    "        ensure_tempme_processed(\n",
    "            dataset,\n",
    "            processed_dir=GRAPHMIXER_PROCESSED_DIR,\n",
    "            resources_datasets=RESOURCES_DATASETS,\n",
    "        )\n",
    "        if not GRAPHMIXER_SCRIPT or not GRAPHMIXER_SCRIPT.exists():\n",
    "            raise FileNotFoundError(f\"GraphMixer training script not found: {GRAPHMIXER_SCRIPT}\")\n",
    "        cmd = build_cmd(PYTHON_BIN, GRAPHMIXER_SCRIPT, get_graphmixer_args(dataset))\n",
    "        code = run_cmd(cmd, env=env, workdir=GRAPHMIXER_SCRIPT.parent, dry_run=DRY_RUN)\n",
    "        if code == 0:\n",
    "            exported = []\n",
    "            if GRAPHMIXER_PARAMS_DIR and GRAPHMIXER_PARAMS_DIR.exists():\n",
    "                dest_dir = RESOURCES_MODELS / slugify(dataset) / \"graphmixer\"\n",
    "                dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "                for src_file in sorted(GRAPHMIXER_PARAMS_DIR.glob(f\"graphmixer_{slugify(dataset)}*.pt\")):\n",
    "                    dest_path = dest_dir / src_file.name\n",
    "                    dest_path.write_bytes(src_file.read_bytes())\n",
    "                    exported.append(dest_path)\n",
    "                if not exported:\n",
    "                    for src_file in sorted(GRAPHMIXER_PARAMS_DIR.glob(\"graphmixer_*.pt\")):\n",
    "                        dest_path = dest_dir / src_file.name\n",
    "                        dest_path.write_bytes(src_file.read_bytes())\n",
    "                        exported.append(dest_path)\n",
    "            if exported:\n",
    "                print(\"[GraphMixer] Exported:\")\n",
    "                for p in exported:\n",
    "                    print(\" -\", p)\n",
    "            else:\n",
    "                print(\"[GraphMixer] No checkpoints found under\", GRAPHMIXER_PARAMS_DIR)\n",
    "            print(\"[GraphMixer] Training completed (or started successfully) for\", dataset)\n",
    "        else:\n",
    "            print(\"[GraphMixer] Training failed for\", dataset)\n",
    "\n",
    "    elif MODEL_TYPE.upper() == \"DBGNN\":\n",
    "        # In-notebook trainer (adapted from 01_train_dbgnn_and_node2vec.ipynb)\n",
    "        if DRY_RUN:\n",
    "            print(\"[DBGNN] DRY_RUN=True -> skip training.\")\n",
    "            continue\n",
    "\n",
    "        # Allow overriding defaults from configs/models/*.json (optional).\n",
    "        # Example:\n",
    "        #   \"models\": { \"DBGNN\": { \"cfg\": { \"epochs\": 200, \"hidden_dims\": [16,32,16] } } }\n",
    "        cfg = dict(DBGNN_DEFAULTS)\n",
    "        try:\n",
    "            cfg.update((MODEL_SPECS.get(\"DBGNN\") or {}).get(\"cfg\") or {})\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        _device = torch.device(DEVICE) if not isinstance(DEVICE, torch.device) else DEVICE\n",
    "        _, _, artifacts = train_dbgnn_node_type(\n",
    "            dataset,\n",
    "            workdir=workdir,\n",
    "            processed_dir=Path(RESOURCES_DATASETS),\n",
    "            device=_device,\n",
    "            cfg=cfg,\n",
    "        )\n",
    "        print(\"[DBGNN] Done. Artifacts:\", artifacts)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"MODEL_TYPE must be 'TGN' or 'TGAT' or 'GRAPHMIXER' or 'DBGNN'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba91cbd-365a-432f-b5ed-4ee86a507808",
   "metadata": {},
   "source": [
    "### TempME pretraining (run once, outside evaluation)\n",
    "Pre-train the TempME base model + explainer here so evaluation notebooks can set `train_if_missing=False` and avoid training inside the explainer pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8702685-8945-4fb2-9124-065772bc23e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ (cwd=/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/explainer/tempme) python /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/explainer/tempme/learn_base.py --base_type tgn --data wikipedia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/explainer/tempme/learn_base.py\", line 106, in <module>\n",
      "    mask_node_set = set(random.sample(set(src_l[ts_l > val_time]).union(set(dst_l[ts_l > val_time])),\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/juliawenkmann/miniconda3/envs/ml/lib/python3.11/random.py\", line 439, in sample\n",
      "    raise TypeError(\"Population must be a sequence.  \"\n",
      "TypeError: Population must be a sequence.  For dicts or sets, use sorted(d).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ERROR] process exited with code 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "TempME base training failed; see logs above.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m     code \u001b[38;5;241m=\u001b[39m run_cmd(cmd, env\u001b[38;5;241m=\u001b[39menv, workdir\u001b[38;5;241m=\u001b[39mTEMP_ME_ROOT, dry_run\u001b[38;5;241m=\u001b[39mDRY_RUN)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 83\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTempME base training failed; see logs above.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expl_ckpt\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[TempME] Explainer checkpoint exists:\u001b[39m\u001b[38;5;124m\"\u001b[39m, expl_ckpt)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: TempME base training failed; see logs above."
     ]
    }
   ],
   "source": [
    "if MODEL_TYPE.upper() == \"DBGNN\":\n",
    "    print(\"Skip TempME pretraining for DBGNN (only relevant for TGNN baselines).\")\n",
    "else:\n",
    "    # --- TempME pretraining (base model + explainer) ---\n",
    "    # Runs in a clean process to avoid memory pressure during evaluation.\n",
    "    TEMP_ME_BASE_TYPE = MODEL_TYPE.lower()\n",
    "    if TEMP_ME_BASE_TYPE not in {\"tgn\", \"tgat\", \"graphmixer\"}:\n",
    "        TEMP_ME_BASE_TYPE = \"tgn\"\n",
    "\n",
    "    TEMP_ME_ROOT = PROJECT_ROOT / \"submodules\" / \"explainer\" / \"tempme\"\n",
    "    TEMP_ME_LEARN_BASE = TEMP_ME_ROOT / \"learn_base.py\"\n",
    "    TEMP_ME_EXP_MAIN = TEMP_ME_ROOT / \"temp_exp_main.py\"\n",
    "\n",
    "    TEMP_ME_CKPT_ROOT = PROJECT_ROOT / \"resources\" / \"explainer\" / \"tempme\"\n",
    "\n",
    "    LEGACY_TEMP_ME_CKPT_ROOT = TEMP_ME_ROOT / \"params\"\n",
    "\n",
    "    # Optional overrides to reduce memory usage. Leave empty to use TempME defaults.\n",
    "    TEMP_ME_BASE_OVERRIDES = {\n",
    "        # \"bs\": 128,\n",
    "        # \"n_epoch\": 50,\n",
    "    }\n",
    "    TEMP_ME_EXP_OVERRIDES = {\n",
    "        # \"bs\": 128,\n",
    "        # \"test_bs\": 128,\n",
    "        # \"n_epoch\": 80,\n",
    "    }\n",
    "\n",
    "    from time_to_explain.data.tgnn_setup import setup_tgnn_data\n",
    "    import shutil\n",
    "\n",
    "    REAL_TGNN_DATASETS = {\"wikipedia\", \"reddit\", \"simulate_v1\", \"simulate_v2\", \"multihost\"}\n",
    "\n",
    "    def _ensure_tempme_inputs(dataset: str) -> None:\n",
    "        missing = []\n",
    "        for fname in (f\"ml_{dataset}.csv\", f\"ml_{dataset}.npy\", f\"ml_{dataset}_node.npy\"):\n",
    "            if not (RESOURCES_DATASETS / fname).exists():\n",
    "                missing.append(fname)\n",
    "        if not missing:\n",
    "            return\n",
    "        print(\"[TempME] Missing processed files, regenerating:\", \", \".join(missing))\n",
    "        if dataset not in REAL_TGNN_DATASETS:\n",
    "            raise ValueError(f\"TempME inputs missing for '{dataset}'. Prepare the dataset or add it to REAL_TGNN_DATASETS.\")\n",
    "        setup_tgnn_data(root=PROJECT_ROOT, only=[dataset], force=False, do_process=True)\n",
    "\n",
    "    def _copy_if_missing(src: Path, dst: Path) -> None:\n",
    "        if not src.exists() or dst.exists():\n",
    "            return\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "        try:\n",
    "            dst.symlink_to(src)\n",
    "        except Exception:\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "    def _maybe_migrate_tempme_ckpts(base_type: str, dataset: str) -> None:\n",
    "        legacy_base = LEGACY_TEMP_ME_CKPT_ROOT / \"tgnn\" / f\"{base_type}_{dataset}.pt\"\n",
    "        legacy_expl = LEGACY_TEMP_ME_CKPT_ROOT / \"explainer\" / base_type / f\"{dataset}.pt\"\n",
    "        new_base, new_expl = _tempme_ckpts(base_type, dataset)\n",
    "        _copy_if_missing(legacy_base, new_base)\n",
    "        _copy_if_missing(legacy_expl, new_expl)\n",
    "\n",
    "    def _tempme_ckpts(base_type: str, dataset: str):\n",
    "        base_ckpt = TEMP_ME_CKPT_ROOT / \"params\" / \"tgnn\" / f\"{base_type}_{dataset}.pt\"\n",
    "        expl_ckpt = TEMP_ME_CKPT_ROOT / \"params\" / \"explainer\" / base_type / f\"{dataset}.pt\"\n",
    "        return base_ckpt, expl_ckpt\n",
    "\n",
    "    env = prepare_env(project_root=PROJECT_ROOT, cuda_visible_devices=CUDA_VISIBLE_DEVICES)\n",
    "\n",
    "    for dataset in DATASET_LIST:\n",
    "        _maybe_migrate_tempme_ckpts(TEMP_ME_BASE_TYPE, dataset)\n",
    "\n",
    "        _ensure_tempme_inputs(dataset)\n",
    "\n",
    "        base_ckpt, expl_ckpt = _tempme_ckpts(TEMP_ME_BASE_TYPE, dataset)\n",
    "\n",
    "        if base_ckpt.exists():\n",
    "            print(\"[TempME] Base checkpoint exists:\", base_ckpt)\n",
    "        else:\n",
    "            base_args = {\"base_type\": TEMP_ME_BASE_TYPE, \"data\": dataset, **TEMP_ME_BASE_OVERRIDES}\n",
    "            cmd = build_cmd(PYTHON_BIN, TEMP_ME_LEARN_BASE, args_dict_to_list(base_args, dataset))\n",
    "            code = run_cmd(cmd, env=env, workdir=TEMP_ME_ROOT, dry_run=DRY_RUN)\n",
    "            if code != 0:\n",
    "                raise RuntimeError(\"TempME base training failed; see logs above.\")\n",
    "\n",
    "        if expl_ckpt.exists():\n",
    "            print(\"[TempME] Explainer checkpoint exists:\", expl_ckpt)\n",
    "        else:\n",
    "            expl_args = {\"base_type\": TEMP_ME_BASE_TYPE, \"data\": dataset, **TEMP_ME_EXP_OVERRIDES}\n",
    "            cmd = build_cmd(PYTHON_BIN, TEMP_ME_EXP_MAIN, args_dict_to_list(expl_args, dataset))\n",
    "            code = run_cmd(cmd, env=env, workdir=TEMP_ME_ROOT, dry_run=DRY_RUN)\n",
    "            if code != 0:\n",
    "                raise RuntimeError(\"TempME explainer training failed; see logs above.\")\n",
    "\n",
    "        print(\"[TempME] Ready for dataset:\", dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95620759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contact-edge check device: cpu\n",
      "Skip wikipedia: contact-edge check is configured for stick_figure/sticky_hips only.\n"
     ]
    }
   ],
   "source": [
    "# --- Contact edge sanity check + visualization (stick_figure + sticky_hips + TGN) ---\n",
    "if DRY_RUN:\n",
    "    print(\"Skip contact-edge check (DRY_RUN=True).\")\n",
    "elif MODEL_TYPE.upper() != \"TGN\":\n",
    "    print(\"Skip contact-edge check (MODEL_TYPE != TGN).\")\n",
    "else:\n",
    "    import json\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import torch\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    from time_to_explain.data.synthetic_recipes.stick_figure import JOINTS_PER_PERSON\n",
    "    from time_to_explain.visualization.utils import COLORS\n",
    "    from submodules.models.tgn.model.tgn import TGN\n",
    "    from submodules.models.tgn.tgn_utils.data_processing import get_data, compute_time_statistics\n",
    "    from submodules.models.tgn.tgn_utils.utils import RandEdgeSampler, get_neighbor_finder\n",
    "\n",
    "    def _find_checkpoint(models_root: Path, dataset_name: str, model_name: str) -> Path:\n",
    "        model_name = model_name.lower()\n",
    "        dataset_name = str(dataset_name)\n",
    "        candidates = [\n",
    "            models_root / dataset_name / model_name / f\"{model_name}_{dataset_name}_best.pth\",\n",
    "            models_root / dataset_name / \"checkpoints\" / f\"{model_name}_{dataset_name}_best.pth\",\n",
    "            models_root / \"checkpoints\" / f\"{model_name}_{dataset_name}_best.pth\",\n",
    "        ]\n",
    "        for cand in candidates:\n",
    "            if cand.exists():\n",
    "                return cand\n",
    "        search_roots = [\n",
    "            models_root / dataset_name / model_name,\n",
    "            models_root / dataset_name,\n",
    "            models_root / \"checkpoints\",\n",
    "        ]\n",
    "        for root in search_roots:\n",
    "            if not root.exists():\n",
    "                continue\n",
    "            matches = sorted(root.rglob(f\"{model_name}*{dataset_name}*.pth\"))\n",
    "            if not matches:\n",
    "                matches = sorted(root.rglob(\"*.pth\"))\n",
    "            for match in matches:\n",
    "                if \"best\" in match.name:\n",
    "                    return match\n",
    "            if matches:\n",
    "                return matches[0]\n",
    "        raise FileNotFoundError(\n",
    "            f\"Checkpoint not found under {models_root} for {model_name}_{dataset_name}.\"\n",
    "        )\n",
    "\n",
    "    def _build_tgn_args(train_args: dict) -> dict:\n",
    "        return {\n",
    "            \"n_layers\": int(train_args.get(\"n_layer\", train_args.get(\"n_layers\", 1))),\n",
    "            \"n_heads\": int(train_args.get(\"n_head\", train_args.get(\"n_heads\", 2))),\n",
    "            \"dropout\": float(train_args.get(\"drop_out\", 0.1)),\n",
    "            \"use_memory\": bool(train_args.get(\"use_memory\", False)),\n",
    "            \"message_dimension\": int(train_args.get(\"message_dim\", 100)),\n",
    "            \"memory_dimension\": int(train_args.get(\"memory_dim\", 172)),\n",
    "            \"memory_update_at_start\": not bool(train_args.get(\"memory_update_at_end\", False)),\n",
    "            \"embedding_module_type\": str(train_args.get(\"embedding_module\", \"graph_attention\")),\n",
    "            \"message_function\": str(train_args.get(\"message_function\", \"identity\")),\n",
    "            \"aggregator_type\": str(train_args.get(\"aggregator\", \"last\")),\n",
    "            \"memory_updater_type\": str(train_args.get(\"memory_updater\", \"gru\")),\n",
    "            \"use_destination_embedding_in_message\": bool(train_args.get(\"use_destination_embedding_in_message\", False)),\n",
    "            \"use_source_embedding_in_message\": bool(train_args.get(\"use_source_embedding_in_message\", False)),\n",
    "            \"dyrep\": bool(train_args.get(\"dyrep\", False)),\n",
    "        }\n",
    "\n",
    "    def _load_processed_tables(dataset: str) -> tuple[pd.DataFrame, np.ndarray, dict]:\n",
    "        flat_csv = RESOURCES_DATASETS / f\"ml_{dataset}.csv\"\n",
    "        if flat_csv.exists():\n",
    "            data_dir = RESOURCES_DATASETS\n",
    "        else:\n",
    "            data_dir = RESOURCES_DATASETS / dataset\n",
    "        graph_df = pd.read_csv(data_dir / f\"ml_{dataset}.csv\")\n",
    "        edge_features = np.load(data_dir / f\"ml_{dataset}.npy\")\n",
    "        meta_path = data_dir / f\"ml_{dataset}.json\"\n",
    "        meta = json.loads(meta_path.read_text(encoding=\"utf-8\")) if meta_path.exists() else {}\n",
    "        return graph_df, edge_features, meta\n",
    "\n",
    "\n",
    "    def _plot_frame(\n",
    "        frame_id: int,\n",
    "        *,\n",
    "        title: str,\n",
    "        probs: dict | None = None,\n",
    "        show_gt: bool = True,\n",
    "        pred_query_only: bool = True,\n",
    "    ) -> go.Figure:\n",
    "        fig = go.Figure()\n",
    "        mask = (clip_ids == clip_id) & (frame_idx == frame_id)\n",
    "        indices = np.where(mask)[0]\n",
    "\n",
    "        if show_gt and probs is not None:\n",
    "            for idx in indices:\n",
    "                if is_query[idx]:\n",
    "                    continue\n",
    "                coords = feat_map[idx]\n",
    "                x0, y0, x1, y1 = [float(v) for v in coords[:4]]\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=[x0, x1],\n",
    "                        y=[y0, y1],\n",
    "                        mode=\"lines\",\n",
    "                        line=dict(color=COLORS[\"base\"], width=1.2),\n",
    "                        opacity=0.35,\n",
    "                        hoverinfo=\"skip\",\n",
    "                        showlegend=False,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        for idx in indices:\n",
    "            coords = feat_map[idx]\n",
    "            x0, y0, x1, y1 = [float(v) for v in coords[:4]]\n",
    "            if probs is None:\n",
    "                if is_query[idx]:\n",
    "                    color = COLORS[\"accent\"]\n",
    "                    width = 3.2\n",
    "                else:\n",
    "                    color = COLORS[\"base\"]\n",
    "                    width = 2.0\n",
    "            else:\n",
    "                if pred_query_only and not is_query[idx]:\n",
    "                    continue\n",
    "                prob = probs.get(int(idx), 0.0)\n",
    "                if prob < 0.5:\n",
    "                    continue\n",
    "                color = COLORS[\"user\"]\n",
    "                width = 2.8\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[x0, x1],\n",
    "                    y=[y0, y1],\n",
    "                    mode=\"lines\",\n",
    "                    line=dict(color=color, width=width),\n",
    "                    hovertemplate=f\"edge_idx={idx_vals[idx]}<extra></extra>\",\n",
    "                    showlegend=False,\n",
    "                )\n",
    "            )\n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            template=\"simple_white\",\n",
    "            xaxis=dict(visible=False),\n",
    "            yaxis=dict(visible=False, scaleanchor=\"x\", scaleratio=1),\n",
    "            margin=dict(l=20, r=20, t=60, b=20),\n",
    "        )\n",
    "        return fig\n",
    "\n",
    "    tgn_spec = MODEL_SPECS.get(\"TGN\", {})\n",
    "    train_args = dict(tgn_spec.get(\"args\") or {})\n",
    "    n_neighbors = int(train_args.get(\"n_degree\", 10))\n",
    "    batch_size = int(train_args.get(\"bs\", 200))\n",
    "    tgn_args = _build_tgn_args(train_args)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Contact-edge check device:\", device)\n",
    "\n",
    "    for dataset in DATASET_LIST:\n",
    "        if dataset not in {\"stick_figure\", \"sticky_hips\"}:\n",
    "            print(f\"Skip {dataset}: contact-edge check is configured for stick_figure/sticky_hips only.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            ckpt_path = _find_checkpoint(RESOURCES_MODELS, dataset, \"tgn\")\n",
    "        except FileNotFoundError as exc:\n",
    "            print(f\"Skip {dataset}: {exc}\")\n",
    "            continue\n",
    "        print(\"Using checkpoint:\", ckpt_path)\n",
    "\n",
    "        node_features, edge_features, full_data, _train_data, _val_data, test_data, _nn_val, _nn_test = get_data(dataset)\n",
    "        m_src, s_src, m_dst, s_dst = compute_time_statistics(\n",
    "            full_data.sources, full_data.destinations, full_data.timestamps\n",
    "        )\n",
    "        full_ngh_finder = get_neighbor_finder(full_data, uniform=False)\n",
    "\n",
    "        model = TGN(\n",
    "            neighbor_finder=full_ngh_finder,\n",
    "            node_features=node_features,\n",
    "            edge_features=edge_features,\n",
    "            device=device,\n",
    "            n_neighbors=n_neighbors,\n",
    "            mean_time_shift_src=m_src,\n",
    "            std_time_shift_src=s_src,\n",
    "            mean_time_shift_dst=m_dst,\n",
    "            std_time_shift_dst=s_dst,\n",
    "            **tgn_args,\n",
    "        )\n",
    "        state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "        filtered_state = {\n",
    "            k: v\n",
    "            for k, v in state_dict.items()\n",
    "            if not (k.startswith(\"memory.\") or k.startswith(\"memory_updater.memory.\"))\n",
    "        }\n",
    "        _ = model.load_state_dict(filtered_state, strict=False)\n",
    "        model = model.to(device).eval()\n",
    "        if getattr(model, \"use_memory\", False) and getattr(model, \"memory\", None) is not None:\n",
    "            model.memory.__init_memory__()\n",
    "\n",
    "        graph_df, edge_feat_full, meta = _load_processed_tables(dataset)\n",
    "        cfg_meta = meta.get(\"config\") if isinstance(meta.get(\"config\"), dict) else {}\n",
    "        frames = int(cfg_meta.get(\"frames\", 30))\n",
    "\n",
    "        idx_col = \"idx\" if \"idx\" in graph_df.columns else (\"e_idx\" if \"e_idx\" in graph_df.columns else None)\n",
    "        if idx_col is None:\n",
    "            print(\"Paired contact check skipped: missing idx/e_idx.\")\n",
    "            continue\n",
    "        idx_vals = graph_df[idx_col].astype(int).to_numpy()\n",
    "        if edge_feat_full.ndim != 2 or edge_feat_full.shape[1] < 9:\n",
    "            print(\"Paired contact check skipped: edge_features missing frame info.\")\n",
    "            continue\n",
    "        if edge_feat_full.shape[0] > int(idx_vals.max()):\n",
    "            feat_map = edge_feat_full[idx_vals]\n",
    "        elif edge_feat_full.shape[0] == len(graph_df):\n",
    "            feat_map = edge_feat_full\n",
    "        else:\n",
    "            print(\"Paired contact check skipped: edge_features length mismatch.\")\n",
    "            continue\n",
    "\n",
    "        u_all = graph_df[\"u\"].astype(int).to_numpy()\n",
    "        v_all = graph_df[\"i\"].astype(int).to_numpy()\n",
    "        ts_all = graph_df[\"ts\"].astype(float).to_numpy()\n",
    "        frame_norm = feat_map[:, 8]\n",
    "        is_query = feat_map[:, 7] >= 0.5\n",
    "\n",
    "        node_min = int(min(u_all.min(), v_all.min()))\n",
    "        node_base = 1 if node_min >= 1 else 0\n",
    "        clip_ids = (u_all - node_base) // JOINTS_PER_PERSON\n",
    "        num_clips = int(clip_ids.max() + 1)\n",
    "        frame_idx = np.clip(np.rint(frame_norm * (frames - 1)), 0, frames - 1).astype(int)\n",
    "\n",
    "        frame_time = np.full((num_clips, frames), np.nan)\n",
    "        contact_present = np.zeros((num_clips, frames), dtype=bool)\n",
    "        for i in range(len(graph_df)):\n",
    "            c = int(clip_ids[i])\n",
    "            f = int(frame_idx[i])\n",
    "            if is_query[i]:\n",
    "                contact_present[c, f] = True\n",
    "            if np.isnan(frame_time[c, f]) or ts_all[i] < frame_time[c, f]:\n",
    "                frame_time[c, f] = ts_all[i]\n",
    "\n",
    "        idx_to_row = {int(v): i for i, v in enumerate(idx_vals.tolist())}\n",
    "        pos_mask = np.asarray(test_data.labels) == 1\n",
    "        pos_edge_idxs = np.asarray(test_data.edge_idxs)[pos_mask]\n",
    "\n",
    "        pos_sources = []\n",
    "        pos_destinations = []\n",
    "        pos_times = []\n",
    "        neg_sources = []\n",
    "        neg_destinations = []\n",
    "        neg_times = []\n",
    "\n",
    "        def _find_non_contact_frame(c: int, f: int) -> int | None:\n",
    "            for delta in range(1, frames):\n",
    "                lo = f - delta\n",
    "                hi = f + delta\n",
    "                if lo >= 0 and not contact_present[c, lo] and not np.isnan(frame_time[c, lo]):\n",
    "                    return lo\n",
    "                if hi < frames and not contact_present[c, hi] and not np.isnan(frame_time[c, hi]):\n",
    "                    return hi\n",
    "            return None\n",
    "\n",
    "        for e_idx in pos_edge_idxs:\n",
    "            row_idx = idx_to_row.get(int(e_idx))\n",
    "            if row_idx is None or not is_query[row_idx]:\n",
    "                continue\n",
    "            c = int(clip_ids[row_idx])\n",
    "            f = int(frame_idx[row_idx])\n",
    "            neg_f = _find_non_contact_frame(c, f)\n",
    "            if neg_f is None:\n",
    "                continue\n",
    "            pos_sources.append(int(u_all[row_idx]))\n",
    "            pos_destinations.append(int(v_all[row_idx]))\n",
    "            pos_times.append(float(ts_all[row_idx]))\n",
    "            neg_sources.append(int(u_all[row_idx]))\n",
    "            neg_destinations.append(int(v_all[row_idx]))\n",
    "            neg_times.append(float(frame_time[c, neg_f]))\n",
    "\n",
    "        if not pos_sources:\n",
    "            print(\"Paired contact check: no usable contact/non-contact pairs found.\")\n",
    "            continue\n",
    "\n",
    "        pos_sources = np.asarray(pos_sources)\n",
    "        pos_destinations = np.asarray(pos_destinations)\n",
    "        pos_times = np.asarray(pos_times)\n",
    "        neg_sources = np.asarray(neg_sources)\n",
    "        neg_destinations = np.asarray(neg_destinations)\n",
    "        neg_times = np.asarray(neg_times)\n",
    "\n",
    "        sampler = RandEdgeSampler(full_data.sources, full_data.destinations, seed=0)\n",
    "\n",
    "        prev_forbidden = getattr(model, \"forbidden_memory_update\", False)\n",
    "        model.forbidden_memory_update = True\n",
    "        if getattr(model, \"use_memory\", False) and getattr(model, \"memory\", None) is not None:\n",
    "            model.memory.__init_memory__()\n",
    "\n",
    "        def _batched_pos_probs(src: np.ndarray, dst: np.ndarray, ts: np.ndarray) -> np.ndarray:\n",
    "            out = []\n",
    "            n = len(src)\n",
    "            for i in range(0, n, batch_size):\n",
    "                s = src[i:i + batch_size]\n",
    "                d = dst[i:i + batch_size]\n",
    "                t = ts[i:i + batch_size]\n",
    "                if len(s) == 0:\n",
    "                    continue\n",
    "                _, neg = sampler.sample(len(s))\n",
    "                edge_idx_dummy = np.zeros(len(s), dtype=int)\n",
    "                pos_prob, _ = model.compute_edge_probabilities(\n",
    "                    s,\n",
    "                    d,\n",
    "                    neg,\n",
    "                    t,\n",
    "                    edge_idx_dummy,\n",
    "                    n_neighbors=n_neighbors,\n",
    "                )\n",
    "                out.append(pos_prob.detach().cpu().numpy())\n",
    "            return np.concatenate(out) if out else np.array([])\n",
    "\n",
    "        pos_prob_pair = _batched_pos_probs(pos_sources, pos_destinations, pos_times)\n",
    "        neg_prob_pair = _batched_pos_probs(neg_sources, neg_destinations, neg_times)\n",
    "\n",
    "        model.forbidden_memory_update = prev_forbidden\n",
    "\n",
    "        pos_hit = int(np.sum(pos_prob_pair >= 0.5))\n",
    "        neg_reject = int(np.sum(neg_prob_pair < 0.5))\n",
    "        total = int(pos_prob_pair.size)\n",
    "        print(f\"Paired contact hit-rate (pos >= 0.5): {100.0 * pos_hit / total:.2f}% ({pos_hit}/{total})\")\n",
    "        print(f\"Paired non-contact reject (neg < 0.5): {100.0 * neg_reject / total:.2f}% ({neg_reject}/{total})\")\n",
    "\n",
    "        # Visualization: predictions at multiple frames\n",
    "        row_idx = None\n",
    "        for e_idx in pos_edge_idxs:\n",
    "            idx = idx_to_row.get(int(e_idx))\n",
    "            if idx is not None and is_query[idx]:\n",
    "                row_idx = idx\n",
    "                break\n",
    "        if row_idx is None:\n",
    "            print(\"Visualization skipped: no contact edge found in test split.\")\n",
    "            continue\n",
    "\n",
    "        clip_id = int(clip_ids[row_idx])\n",
    "        frame_contact = int(frame_idx[row_idx])\n",
    "        pred_frames = 8\n",
    "        start_frame = max(0, frame_contact - (pred_frames // 2))\n",
    "        end_frame = min(frames - 1, start_frame + pred_frames - 1)\n",
    "        start_frame = max(0, end_frame - (pred_frames - 1))\n",
    "        display_frames = list(range(start_frame, end_frame + 1))\n",
    "\n",
    "        prev_forbidden = getattr(model, \"forbidden_memory_update\", False)\n",
    "        model.forbidden_memory_update = False\n",
    "        if getattr(model, \"use_memory\", False) and getattr(model, \"memory\", None) is not None:\n",
    "            model.memory.__init_memory__()\n",
    "\n",
    "        frame_probs: dict[int, dict[int, float]] = {}\n",
    "        for f in range(frames):\n",
    "            indices_f = np.where((clip_ids == clip_id) & (frame_idx == f))[0]\n",
    "            if len(indices_f) == 0:\n",
    "                continue\n",
    "            s = u_all[indices_f]\n",
    "            d = v_all[indices_f]\n",
    "            t = ts_all[indices_f]\n",
    "            e = idx_vals[indices_f]\n",
    "            _, neg = sampler.sample(len(s))\n",
    "            pos_prob, _ = model.compute_edge_probabilities(\n",
    "                s,\n",
    "                d,\n",
    "                neg,\n",
    "                t,\n",
    "                e,\n",
    "                n_neighbors=n_neighbors,\n",
    "            )\n",
    "            if f in display_frames:\n",
    "                probs = {\n",
    "                    int(idx): float(p)\n",
    "                    for idx, p in zip(indices_f, pos_prob.detach().cpu().numpy())\n",
    "                }\n",
    "                frame_probs[int(f)] = probs\n",
    "\n",
    "        model.forbidden_memory_update = prev_forbidden\n",
    "\n",
    "        if not frame_probs:\n",
    "            print(\"Visualization skipped: no frames selected.\")\n",
    "            continue\n",
    "\n",
    "        for f in display_frames:\n",
    "            probs = frame_probs.get(int(f))\n",
    "            if not probs:\n",
    "                continue\n",
    "            contact_flag = \"contact\" if contact_present[clip_id, f] else \"no contact\"\n",
    "            fig_pred = _plot_frame(\n",
    "                f,\n",
    "                title=f\"{dataset}: predictions at frame {f} ({contact_flag})\",\n",
    "                probs=probs,\n",
    "                show_gt=True,\n",
    "                pred_query_only=True,\n",
    "            )\n",
    "            fig_pred.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
