{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate Explainers\n",
    "\n",
    "With all these prerequisites out of the way you can now run the experiments themselves. The experiments are run for each\n",
    "explanation method (T-GNNExplainer, GreDyCF, CoDy), for each dataset, for each correct/incorrect setting \n",
    "(correct predictions only/incorrect predictions only), and for each selection policy (random, temporal, spatio-temporal, \n",
    "local-gradient) separately. For convenience, all selection strategies can be automatically evaluated in parallel from a \n",
    "single script. An additional feature of the evaluation is that it can be interrupted by Keyboard Interruption or by the\n",
    "maximum processing time. When the evaluation is interrupted before it is finished, the intermediary results are saved. \n",
    "The evaluation automatically resumes from intermediary results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using helpers from: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/notebooks/src\n",
      "utils.utils   -> ModuleSpec(name='utils.utils', loader=<_frozen_importlib_external.SourceFileLoader object at 0x10bb46790>, origin='/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/models/tgn/utils/utils.py')\n",
      "modules.memory-> ModuleSpec(name='modules.memory', loader=<_frozen_importlib_external.SourceFileLoader object at 0x10bb47d90>, origin='/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/models/tgn/modules/memory.py')\n",
      "REPO_ROOT        : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain\n",
      "PKG_DIR          : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/time_to_explain\n",
      "RESOURCES_DIR    : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources\n",
      "PROCESSED_DATA_DIR: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/datasets/processed\n",
      "MODELS_ROOT      : /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/resources/models\n"
     ]
    }
   ],
   "source": [
    "# Find and add `notebooks/src` to sys.path, no matter where the notebook lives.\n",
    "from pathlib import Path\n",
    "import sys, importlib\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def _add_notebooks_src_to_path():\n",
    "    here = Path.cwd().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        candidate = p / \"notebooks\" / \"src\"\n",
    "        if candidate.is_dir():\n",
    "            if str(candidate) not in sys.path:\n",
    "                sys.path.insert(0, str(candidate))\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Could not find 'notebooks/src' from current working directory.\")\n",
    "\n",
    "print(\"Using helpers from:\", _add_notebooks_src_to_path())\n",
    "\n",
    "from constants import (\n",
    "    REPO_ROOT, PKG_DIR, RESOURCES_DIR, PROCESSED_DATA_DIR, MODELS_ROOT, TGN_SUBMODULE_ROOT, ensure_repo_importable, get_last_checkpoint\n",
    ")\n",
    "ensure_repo_importable()\n",
    "from device import pick_device\n",
    "\n",
    "for p in (str(TGN_SUBMODULE_ROOT), str(REPO_ROOT), str(PKG_DIR)):\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "# 2) If your notebook already imported `utils`, remove it to avoid collision\n",
    "if \"utils\" in sys.modules:\n",
    "    del sys.modules[\"utils\"]\n",
    "\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "# 4) (Optional) sanity check that TGN's local packages resolve\n",
    "import importlib.util as iu\n",
    "print(\"utils.utils   ->\", iu.find_spec(\"utils.utils\"))\n",
    "print(\"modules.memory->\", iu.find_spec(\"modules.memory\"))\n",
    "\n",
    "# 5) Now this import should work without the previous error\n",
    "from time_to_explain.models.wrapper import (\n",
    "    create_dataset, create_tgn_wrapper, create_wrapper, create_tgat_wrapper\n",
    ")\n",
    "\n",
    "print(\"REPO_ROOT        :\", REPO_ROOT)\n",
    "print(\"PKG_DIR          :\", PKG_DIR)\n",
    "print(\"RESOURCES_DIR    :\", RESOURCES_DIR)\n",
    "print(\"PROCESSED_DATA_DIR:\", PROCESSED_DATA_DIR)\n",
    "print(\"MODELS_ROOT      :\", MODELS_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting:\n",
    "\n",
    "Replace ``MODEL-TYPE`` with the type of the model you want to evaluate, e.g., 'TGAT' or 'TGN'.\n",
    "\n",
    "Replace ``DATASET-NAME`` with the name of the dataset on which you want to train the PGExplainer model, e.g., 'uci', \n",
    "'wikipedia', etc.\n",
    "\n",
    "Replace ``EXPLAINER-NAME`` with the explainer you want to evaluate. Options are ``tgnnexplainer``, ``greedy``, ``cody``.\n",
    "\n",
    "Replace ``SELECTION-NAME`` with the selection policy that you want to evaluate. The options are ``random``, \n",
    "``temporal``, ``spatio-temporal``, ``local-gradient``, and ``all``. Use the ``all`` option to efficiently evaluate the\n",
    "different selection strategies with caching between selection strategies.\n",
    "**Do not provide a `SELECTION-NAME`` argument when evaluating T-GNNExplainer**\n",
    "\n",
    "Replace ``TIME-LIMIT`` with an integer number that sets a limit on the maximum time that the evaluation runs before \n",
    "concluding in minutes. The evaluation can be resumed from that state at a later time.\n",
    "\n",
    "Only set ``bipartite = True``  if the underlying dataset is a bipartite graph (Wikipedia/UCI-Forums).\n",
    "\n",
    "As an example, to run the evaluation of CoDy for all selection strategies, with a time limit of 240 minutes and the\n",
    "bipartite wikipedia dataset, the following command is used:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = \"TGAT\"\n",
    "DATASET_NAME = \"wikipedia\"\n",
    "EXPLAINER = \"cody\"\n",
    "SELECTION_NAME = \"all\"\n",
    "TIME_LIMIT = 240\n",
    "BIPARTITE = True\n",
    "\n",
    "DIRECTED = False\n",
    "EPOCHS = 10\n",
    "\n",
    "MODEL_PATH = MODELS_ROOT / DATASET_NAME\n",
    "CHECKPOINT_PATH = MODEL_PATH / 'checkpoints/'\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    os.mkdir(CHECKPOINT_PATH)\n",
    "LAST_CHECKPOINT = get_last_checkpoint(CHECKPOINT_PATH,MODEL_TYPE, DATASET_NAME)    \n",
    "DEVICE = pick_device(\"auto\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate explanations for correct predictions only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate explanations for incorrect predictions only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
