{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocess the evaluation results\n",
    "\n",
    "```shell\n",
    "python evaluate_factual_subgraphs.py \n",
    "  -d ../resources/datasets/processed/wikipedia \n",
    "  --cuda \n",
    "  --bipartite \n",
    "  --model ../resources/models/wikipedia/wikipedia-wikipedia.pth \n",
    "  --type TGAT\n",
    "  --results ../resources/results/wikipedia/tgnnexplainer/results_wikipedia_tgnnexplainer.parquet\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```shell\n",
    "python evaluate_fidelity_minus.py\n",
    "  -d ../resources/datasets/processed/wikipedia\n",
    "  --cuda\n",
    "  --bipartite\n",
    "  --model ../resources/models/wikipedia/wikipedia-wikipedia.pth\n",
    "  --type TGAT\n",
    "  --results ../results/wikipedia/greedy\n",
    "  --all_files_in_dir\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using helpers from: /Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/notebooks/src\n",
      "utils.utils   -> ModuleSpec(name='utils.utils', loader=<_frozen_importlib_external.SourceFileLoader object at 0x11352f610>, origin='/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/models/tgn/utils/utils.py')\n",
      "modules.memory-> ModuleSpec(name='modules.memory', loader=<_frozen_importlib_external.SourceFileLoader object at 0x11355a050>, origin='/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/submodules/models/tgn/modules/memory.py')\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'add_wrapper_model_arguments' from 'time_to_explain.data.common' (/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/time_to_explain/data/common.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodules.memory->\u001b[39m\u001b[38;5;124m\"\u001b[39m, iu\u001b[38;5;241m.\u001b[39mfind_spec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodules.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 5) Now this import should work without the previous error\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime_to_explain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate_factual_subgraphs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     42\u001b[0m     evaluate_explainer_results\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREPO_ROOT        :\u001b[39m\u001b[38;5;124m\"\u001b[39m, REPO_ROOT)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPKG_DIR          :\u001b[39m\u001b[38;5;124m\"\u001b[39m, PKG_DIR)\n",
      "File \u001b[0;32m~/Documents/CodingProjects/master_thesis/time_to_explain/time_to_explain/explainer/evaluate_factual_subgraphs.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime_to_explain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainTestDatasetParameters\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime_to_explain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msetup\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProgressBar\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime_to_explain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     add_dataset_arguments,\n\u001b[1;32m     15\u001b[0m     add_wrapper_model_arguments,\n\u001b[1;32m     16\u001b[0m     create_dataset_from_args,\n\u001b[1;32m     17\u001b[0m     create_tgnn_wrapper_from_args,\n\u001b[1;32m     18\u001b[0m     parse_args,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluate_explainer_results\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse_results_cell\u001b[39m(cell: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[\u001b[38;5;28mdict\u001b[39m]:\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'add_wrapper_model_arguments' from 'time_to_explain.data.common' (/Users/juliawenkmann/Documents/CodingProjects/master_thesis/time_to_explain/time_to_explain/data/common.py)"
     ]
    }
   ],
   "source": [
    "# Find and add `notebooks/src` to sys.path, no matter where the notebook lives.\n",
    "from pathlib import Path\n",
    "import sys, importlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "\n",
    "def _add_notebooks_src_to_path():\n",
    "    here = Path.cwd().resolve()\n",
    "    for p in [here, *here.parents]:\n",
    "        candidate = p / \"notebooks\" / \"src\"\n",
    "        if candidate.is_dir():\n",
    "            if str(candidate) not in sys.path:\n",
    "                sys.path.insert(0, str(candidate))\n",
    "            return candidate\n",
    "    raise FileNotFoundError(\"Could not find 'notebooks/src' from current working directory.\")\n",
    "\n",
    "print(\"Using helpers from:\", _add_notebooks_src_to_path())\n",
    "\n",
    "from constants import (\n",
    "    REPO_ROOT, PKG_DIR, RESOURCES_DIR, PROCESSED_DATA_DIR, MODELS_ROOT, RESULTS_ROOT, TGN_SUBMODULE_ROOT, ensure_repo_importable, get_last_checkpoint\n",
    ")\n",
    "ensure_repo_importable()\n",
    "from device import pick_device\n",
    "\n",
    "for p in (str(TGN_SUBMODULE_ROOT), str(REPO_ROOT), str(PKG_DIR)):\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "# 2) If your notebook already imported `utils`, remove it to avoid collision\n",
    "if \"utils\" in sys.modules:\n",
    "    del sys.modules[\"utils\"]\n",
    "\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "# 4) (Optional) sanity check that TGN's local packages resolve\n",
    "import importlib.util as iu\n",
    "print(\"utils.utils   ->\", iu.find_spec(\"utils.utils\"))\n",
    "print(\"modules.memory->\", iu.find_spec(\"modules.memory\"))\n",
    "\n",
    "# 5) Now this import should work without the previous error\n",
    "from time_to_explain.explainer.evaluate_factual_subgraphs import (\n",
    "    evaluate_explainer_results\n",
    ")\n",
    "from time_to_explain.metrics.evaluate_fidelity_minus import (\n",
    "    add_prediction_for_dataframe, save_results_to_file\n",
    ")\n",
    "\n",
    "print(\"REPO_ROOT        :\", REPO_ROOT)\n",
    "print(\"PKG_DIR          :\", PKG_DIR)\n",
    "print(\"RESOURCES_DIR    :\", RESOURCES_DIR)\n",
    "print(\"PROCESSED_DATA_DIR:\", PROCESSED_DATA_DIR)\n",
    "print(\"MODELS_ROOT      :\", MODELS_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE = \"TGAT\"\n",
    "DATASET_NAME = \"wikipedia\"\n",
    "EXPLAINER = \"cody\"\n",
    "SELECTION_NAME = \"all\"\n",
    "BIPARTITE = True\n",
    "\n",
    "\n",
    "MODEL_PATH = MODELS_ROOT / DATASET_NAME\n",
    "RESULTS_PATH = RESULTS_ROOT / DATASET_NAME /EXPLAINER \n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "    os.makedirs(RESULTS_PATH)\n",
    "RESULT_FILE = RESULTS_PATH / f\"results_{DATASET_NAME}_{EXPLAINER}.parquet\"\n",
    "\n",
    "ALL_FILES_IN_DIR = True\n",
    "\n",
    "\n",
    "DEVICE = pick_device(\"auto\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish the evaluation and enrich the evaluation datasets with further information we need 2 more steps:\n",
    "\n",
    "## Enrich Factual Explanations:\n",
    "\n",
    "The fist step is to add the prediction that is achieved when removing the events in the explanation produced by \n",
    "T-GNNExplainer from the input data (to evaluate fidelity+/whether the explanation is counterfactual). To add this \n",
    "information, use the [evaluate_factual_subgraphs.py](/scripts/evaluate_factual_subgraphs.py) script. This adds the information to the results for correct predictions only for the wikipedia dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset_from_args(\n",
    "    args,\n",
    "    TrainTestDatasetParameters(0.2, 0.6, 0.8, 500, 500, 500),\n",
    ")\n",
    "tgn_wrapper = create_tgnn_wrapper(args, dataset)\n",
    "\n",
    "# Call the function using variables (no parsing inside the function)\n",
    "evaluate_explainer_results(\n",
    "    results_path=RESULT_FILE,\n",
    "    tgn_wrapper=tgn_wrapper,\n",
    "    show_progress=True,\n",
    "    drop_first_csv_col=True,  # preserves the original behavior on CSV input\n",
    "    save_outputs=True,\n",
    "    save_csv=True,\n",
    "    save_parquet=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Fidelity- information:\n",
    "\n",
    "The second step is to analyze how much the events in the explanation influence the prediction. For this, use the \n",
    "[evaluate_fidelity_minus.py](/scripts/evaluate_fidelity_minus.py). This adds the fidelity- information to all the results saved in the _results/wikipedia/greedy_ directory. Rerun the \n",
    "command for each of the explainers and each of the datasets to add the information to all results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgn_wrapper.set_evaluation_mode(True)\n",
    "\n",
    "if not os.path.exists(RESULT_FILE):\n",
    "    raise FileNotFoundError('Failed to locate the file containing the results')\n",
    "\n",
    "dataframes = []\n",
    "if ALL_FILES_IN_DIR:\n",
    "    results_files = [f for f in listdir(RESULTS_PATH) if os.path.isfile(os.path.join(RESULTS_PATH, f)) and f.endswith('parquet')]\n",
    "    for results_file in results_files:\n",
    "        filepath = f'{RESULTS_PATH}/{results_file}'\n",
    "        results = pd.read_parquet(filepath)\n",
    "        dataframes.append((results, filepath.rstrip('parquet')))\n",
    "else:\n",
    "    if RESULT_FILE.endswith('parquet'):\n",
    "        results = pd.read_parquet(RESULT_FILE)\n",
    "        dataframes.append((results, RESULT_FILE.rstrip('parquet')))\n",
    "    else:\n",
    "        raise RuntimeError('Cannot read results. Only parquet files supported.')\n",
    "\n",
    "for results, filename in dataframes:\n",
    "    results = add_prediction_for_dataframe(results, tgn_wrapper)\n",
    "    save_results_to_file(results, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
