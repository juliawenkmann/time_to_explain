#!/usr/bin/env python3
"""
Real TGNN dataset setup (download + optional processing).

This module handles *real* datasets that are not generated by synthetic recipes.
It is intentionally small and composable; the high-level orchestration lives in
`tgnn_prepare.prepare_tgnn_dataset()`.

Supported datasets (by name)
----------------------------
- wikipedia, reddit: SNAP/JODIE CSVs (download raw -> process to ml_*.csv + .npy)
- simulate_v1, simulate_v2: pre-processed files (download ml_*.csv + .npy + _node.npy)
- multihost: TemGX supplementary material from OpenReview (download zip -> extract dataset files)

All outputs use the flat resources layout:
  resources/datasets/raw/<name>.csv
  resources/datasets/processed/ml_<name>.csv
  resources/datasets/processed/ml_<name>.npy
  resources/datasets/processed/ml_<name>_node.npy

Notes on TemGX supplementary
----------------------------
The TemGX submission (OpenReview) provides a supplementary ZIP. We download it from:
  https://openreview.net/attachment?id=<NOTE_ID>&name=supplementary_material

Then we search inside for files matching the requested dataset (e.g. "multihost").
If ml_* processed files are found, we copy them directly. Otherwise we fall back to
a generic CSV->ml_ processing step.

If OpenReview blocks anonymous user agents, set a browser-like User-Agent header
(which we do by default), or install openreview-py and extend the downloader.
"""
from __future__ import annotations

import argparse
import os
import shutil
import urllib.request
import zipfile
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, Optional, Sequence

import numpy as np
import pandas as pd

from time_to_explain.data.validate import verify_dataframe_unify


# -----------------------------------------------------------------------------
# URLs / identifiers
# -----------------------------------------------------------------------------

SNAP_JODIE = {
    "wikipedia": "http://snap.stanford.edu/jodie/wikipedia.csv",
    "reddit": "http://snap.stanford.edu/jodie/reddit.csv",
}

HAWKES_SIM = {
    # raw CSVs (optional)
    "simulate_v1": "https://m-krastev.github.io/hawkes-sim-datasets/simulate_v1.csv",
    "simulate_v2": "https://m-krastev.github.io/hawkes-sim-datasets/simulate_v2.csv",
    # pre-processed TGAT-style files
    "ml_simulate_v1.csv": "https://m-krastev.github.io/hawkes-sim-datasets/ml_simulate_v1.csv",
    "ml_simulate_v1.npy": "https://m-krastev.github.io/hawkes-sim-datasets/ml_simulate_v1.npy",
    "ml_simulate_v1_node.npy": "https://m-krastev.github.io/hawkes-sim-datasets/ml_simulate_v1_node.npy",
    "ml_simulate_v2.csv": "https://m-krastev.github.io/hawkes-sim-datasets/ml_simulate_v2.csv",
    "ml_simulate_v2.npy": "https://m-krastev.github.io/hawkes-sim-datasets/ml_simulate_v2.npy",
    "ml_simulate_v2_node.npy": "https://m-krastev.github.io/hawkes-sim-datasets/ml_simulate_v2_node.npy",
}

# TemGX OpenReview submission id (supplementary material zip lives here)
TEMGX_OPENREVIEW_NOTE_ID = "NqtYz3A8tQ"
TEMGX_ATTACHMENT_NAME = "supplementary_material"


# -----------------------------------------------------------------------------
# Path helpers
# -----------------------------------------------------------------------------

def resolve_root(root: Optional[os.PathLike] = None) -> Path:
    """Resolve repository root. Priority: explicit arg -> env vars -> CWD."""
    if root:
        return Path(root).expanduser().resolve()
    for key in ("ROOT", "REPO_ROOT", "PROJECT_ROOT"):
        if key in os.environ and os.environ[key].strip():
            return Path(os.environ[key]).expanduser().resolve()
    return Path.cwd().resolve()


def ensure_dir(p: os.PathLike) -> Path:
    p = Path(p)
    p.mkdir(parents=True, exist_ok=True)
    return p


@dataclass(frozen=True)
class RealDataDirs:
    raw_dir: Path
    processed_dir: Path

    @classmethod
    def from_root(cls, root: Path, *, raw_dir: Optional[os.PathLike] = None, processed_dir: Optional[os.PathLike] = None) -> "RealDataDirs":
        raw = Path(raw_dir) if raw_dir is not None else (root / "resources" / "datasets" / "raw")
        proc = Path(processed_dir) if processed_dir is not None else (root / "resources" / "datasets" / "processed")
        ensure_dir(raw)
        ensure_dir(proc)
        return cls(raw_dir=raw, processed_dir=proc)


# -----------------------------------------------------------------------------
# Download helpers
# -----------------------------------------------------------------------------

def download_url(url: str, dest: Path, *, force: bool = False, user_agent: Optional[str] = None) -> None:
    dest = Path(dest)
    if dest.exists() and dest.stat().st_size > 0 and not force:
        return

    print(f"↓ Downloading {url} -> {dest}")
    dest.parent.mkdir(parents=True, exist_ok=True)

    headers = {}
    if user_agent:
        headers["User-Agent"] = user_agent
    else:
        # A conservative UA string; some hosts block urllib's default.
        headers["User-Agent"] = "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36"

    req = urllib.request.Request(url, headers=headers)
    with urllib.request.urlopen(req) as resp, dest.open("wb") as f:
        shutil.copyfileobj(resp, f)


def download_openreview_supplementary_zip(dest: Path, *, force: bool = False, note_id: str = TEMGX_OPENREVIEW_NOTE_ID) -> Path:
    url = f"https://openreview.net/attachment?id={note_id}&name={TEMGX_ATTACHMENT_NAME}"
    download_url(url, dest, force=force)
    return dest


def _extract_zip(zip_path: Path, extract_dir: Path, *, force: bool = False) -> Path:
    zip_path = Path(zip_path)
    extract_dir = Path(extract_dir)
    if extract_dir.exists() and any(extract_dir.rglob("*")) and not force:
        return extract_dir
    if extract_dir.exists():
        shutil.rmtree(extract_dir)
    extract_dir.mkdir(parents=True, exist_ok=True)
    print(f"↪ Extracting {zip_path} -> {extract_dir}")
    with zipfile.ZipFile(zip_path, "r") as zf:
        zf.extractall(extract_dir)
    return extract_dir


def _find_first(root: Path, *, predicate) -> Optional[Path]:
    for p in root.rglob("*"):
        if p.is_file() and predicate(p):
            return p
    return None


def _find_all(root: Path, *, predicate) -> list[Path]:
    out: list[Path] = []
    for p in root.rglob("*"):
        if p.is_file() and predicate(p):
            out.append(p)
    return out


# -----------------------------------------------------------------------------
# Processing
# -----------------------------------------------------------------------------

def _parse_feature_column(df: pd.DataFrame, col: str) -> np.ndarray:
    """
    Parse a 'comma_separated_list_of_features' string column into a float array.
    """
    feats = df[col].astype(str).str.split(",", expand=True)
    feats = feats.apply(pd.to_numeric, errors="coerce").fillna(0.0)
    return feats.to_numpy(dtype=np.float32)


def _infer_feature_cols(df: pd.DataFrame) -> list[str]:
    required = {"u", "i", "ts", "label", "idx", "e_idx"}
    # f0, f1, ... (preferred)
    fcols = [c for c in df.columns if isinstance(c, str) and c.startswith("f") and c[1:].isdigit()]
    if fcols:
        return fcols
    # otherwise: numeric columns not in required
    numeric = [c for c in df.columns if c not in required and pd.api.types.is_numeric_dtype(df[c])]
    return numeric


def process_csv_to_ml(
    dataset_name: str,
    raw_csv: Path,
    *,
    out_dir: Path,
    bipartite: bool,
    node_feat_dim: Optional[int] = None,
) -> None:
    """
    Generic CSV -> TGAT-style processed files.

    Expected minimal columns: u, i, ts, label
    Optional feature columns:
      - comma_separated_list_of_features (SNAP/JODIE style)
      - f0..fK-1
      - any other numeric columns (treated as features)
    """
    raw_csv = Path(raw_csv)
    out_dir = ensure_dir(out_dir)

    df = pd.read_csv(raw_csv)
    # Common alternative names (best-effort)
    rename_map = {}
    for cand in ("src", "source", "from"):
        if cand in df.columns and "u" not in df.columns:
            rename_map[cand] = "u"
            break
    for cand in ("dst", "dest", "destination", "to"):
        if cand in df.columns and "i" not in df.columns:
            rename_map[cand] = "i"
            break
    if "time" in df.columns and "ts" not in df.columns:
        rename_map["time"] = "ts"
    if rename_map:
        df = df.rename(columns=rename_map)

    missing = [c for c in ("u", "i", "ts", "label") if c not in df.columns]
    if missing:
        raise ValueError(f"{dataset_name}: raw CSV missing columns {missing}. Columns={list(df.columns)}")

    # Features
    if "comma_separated_list_of_features" in df.columns:
        edge_feats = _parse_feature_column(df, "comma_separated_list_of_features")
    else:
        feat_cols = _infer_feature_cols(df)
        edge_feats = df[feat_cols].to_numpy(dtype=np.float32) if feat_cols else np.zeros((len(df), 0), dtype=np.float32)

    # Sort by time
    df = df[["u", "i", "ts", "label"]].copy()
    df["ts"] = df["ts"].astype(float)
    df = df.sort_values("ts").reset_index(drop=True)

    # Reindex nodes to 1..N
    if bipartite:
        u_codes, u_uniques = pd.factorize(df["u"], sort=True)
        i_codes, i_uniques = pd.factorize(df["i"], sort=True)
        u = u_codes.astype(np.int64) + 1
        i = i_codes.astype(np.int64) + 1 + len(u_uniques)
    else:
        all_nodes = pd.concat([df["u"], df["i"]], axis=0)
        codes, uniques = pd.factorize(all_nodes, sort=True)
        u = codes[: len(df)].astype(np.int64) + 1
        i = codes[len(df) :].astype(np.int64) + 1

    df["u"] = u
    df["i"] = i
    df["label"] = df["label"].astype(int)

    df["idx"] = np.arange(1, len(df) + 1, dtype=np.int64)
    df["e_idx"] = df["idx"]

    verify_dataframe_unify(df, bipartite=bipartite)

    # Save ml_*.csv
    out_csv = out_dir / f"ml_{dataset_name}.csv"
    df.to_csv(out_csv, index=False)

    # Save edge features with padding row 0
    out_edge = out_dir / f"ml_{dataset_name}.npy"
    edge_feats = edge_feats.astype(np.float32)
    edge_padded = np.zeros((len(df) + 1, edge_feats.shape[1]), dtype=np.float32)
    if edge_feats.shape[1] > 0:
        edge_padded[1:] = edge_feats[: len(df)]
    np.save(out_edge, edge_padded)

    # Save node features (zeros) with padding row 0
    out_node = out_dir / f"ml_{dataset_name}_node.npy"
    num_nodes = int(max(df["u"].max(), df["i"].max()))
    d = int(node_feat_dim) if node_feat_dim is not None else int(edge_padded.shape[1])
    node = np.zeros((num_nodes + 1, d), dtype=np.float32)
    np.save(out_node, node)


def process_snap_jodie(dataset_name: str, raw_csv: Path, *, out_dir: Path) -> None:
    """
    SNAP/JODIE wikipedia/reddit format:
      u,i,ts,label,comma_separated_list_of_features
    (u and i are 0-based and in separate id spaces -> bipartite=True)
    """
    process_csv_to_ml(dataset_name, raw_csv, out_dir=out_dir, bipartite=True)


# -----------------------------------------------------------------------------
# TemGX supplementary extraction
# -----------------------------------------------------------------------------

def _copy(src: Path, dst: Path, *, force: bool) -> None:
    dst.parent.mkdir(parents=True, exist_ok=True)
    if dst.exists() and not force:
        return
    shutil.copy2(src, dst)


def ensure_temgx_dataset(
    dataset_name: str,
    *,
    dirs: RealDataDirs,
    force: bool = False,
    note_id: str = TEMGX_OPENREVIEW_NOTE_ID,
) -> None:
    """
    Ensure a TemGX dataset (e.g. multihost) is available in our resources layout.

    Strategy:
      1) download supplementary zip (cached)
      2) extract it (cached)
      3) search for processed ml_* files OR a raw CSV containing the dataset_name
      4) copy into resources; if only raw CSV is found, run generic processing
    """
    raw_dir, proc_dir = dirs.raw_dir, dirs.processed_dir
    supp_zip = raw_dir / f"temgx_{note_id}.zip"
    download_openreview_supplementary_zip(supp_zip, force=force, note_id=note_id)

    extract_dir = raw_dir / f"_temgx_{note_id}"
    _extract_zip(supp_zip, extract_dir, force=force)

    key = dataset_name.lower().replace("-", "").replace("_", "")
    def has_key(p: Path) -> bool:
        s = p.name.lower().replace("-", "").replace("_", "")
        return key in s

    # 1) Prefer fully processed ml_* files
    ml_csv = _find_first(extract_dir, predicate=lambda p: p.suffix == ".csv" and has_key(p) and p.name.lower().startswith("ml_"))
    ml_edge = _find_first(extract_dir, predicate=lambda p: p.suffix == ".npy" and has_key(p) and p.name.lower().startswith("ml_") and not p.name.lower().endswith("_node.npy"))
    ml_node = _find_first(extract_dir, predicate=lambda p: p.suffix == ".npy" and has_key(p) and p.name.lower().startswith("ml_") and p.name.lower().endswith("_node.npy"))

    if ml_csv is not None:
        _copy(ml_csv, proc_dir / f"ml_{dataset_name}.csv", force=force)
        if ml_edge is not None:
            _copy(ml_edge, proc_dir / f"ml_{dataset_name}.npy", force=force)
        if ml_node is not None:
            _copy(ml_node, proc_dir / f"ml_{dataset_name}_node.npy", force=force)
        return

    # 2) Otherwise: copy a raw CSV and process it
    raw_csv = _find_first(extract_dir, predicate=lambda p: p.suffix == ".csv" and has_key(p))
    if raw_csv is None:
        # Helpful debug: show a few candidates
        candidates = _find_all(extract_dir, predicate=lambda p: p.suffix in {".csv", ".npy"} and ("host" in p.name.lower() or "multi" in p.name.lower()))
        preview = "\n".join(f"  - {c.relative_to(extract_dir)}" for c in candidates[:20])
        raise FileNotFoundError(
            f"Could not find files for dataset '{dataset_name}' inside TemGX supplementary zip.\n"
            f"Looked under: {extract_dir}\n"
            f"Example candidates:\n{preview}\n"
        )

    dst_raw = raw_dir / f"{dataset_name}.csv"
    _copy(raw_csv, dst_raw, force=force)

    # TemGX multihost is non-bipartite (shared node id space)
    process_csv_to_ml(dataset_name, dst_raw, out_dir=proc_dir, bipartite=False)


# -----------------------------------------------------------------------------
# Public API
# -----------------------------------------------------------------------------

def ensure_real_dataset(
    dataset_name: str,
    *,
    root: Optional[os.PathLike] = None,
    force: bool = False,
    do_process: bool = True,
    raw_dir: Optional[os.PathLike] = None,
    processed_dir: Optional[os.PathLike] = None,
) -> None:
    """
    Ensure a real dataset is present in resources/ and processed to TGAT format.

    This is intentionally idempotent unless `force=True`.
    """
    root_path = resolve_root(root)
    dirs = RealDataDirs.from_root(root_path, raw_dir=raw_dir, processed_dir=processed_dir)

    name = dataset_name.strip().lower()

    # SNAP/JODIE (raw -> processed)
    if name in SNAP_JODIE:
        raw_csv = dirs.raw_dir / f"{name}.csv"
        download_url(SNAP_JODIE[name], raw_csv, force=force)
        if do_process:
            process_snap_jodie(name, raw_csv, out_dir=dirs.processed_dir)
        return

    # Simulated (pre-processed)
    if name in {"simulate_v1", "simulate_v2"}:
        # optional raw
        raw_csv = dirs.raw_dir / f"{name}.csv"
        if name in HAWKES_SIM:
            download_url(HAWKES_SIM[name], raw_csv, force=force)

        # processed
        for fname in (f"ml_{name}.csv", f"ml_{name}.npy", f"ml_{name}_node.npy"):
            url = HAWKES_SIM.get(fname)
            if url is None:
                continue
            download_url(url, dirs.processed_dir / fname, force=force)
        return

    # TemGX supplementary
    if name in {"multihost"}:
        ensure_temgx_dataset(name, dirs=dirs, force=force)
        return

    raise ValueError(
        f"Unknown real dataset '{dataset_name}'. Supported: "
        f"{sorted(set(SNAP_JODIE) | {'simulate_v1','simulate_v2','multihost'})}"
    )


# Backwards compatible name: used by older code
def setup_tgnn_data(
    root: Optional[os.PathLike] = None,
    only: Optional[Sequence[str]] = None,
    force: bool = False,
    do_process: bool = True,
    do_index: bool = False,
    seed: int = 42,
    index_size: int = 500,
    data_dir: Optional[os.PathLike] = None,
    proc_dir: Optional[os.PathLike] = None,
    idx_dir: Optional[os.PathLike] = None,
) -> None:
    """
    Legacy-compatible wrapper around `ensure_real_dataset`.

    NOTE: `do_index`, `seed`, `index_size`, `idx_dir` are kept only for signature
    compatibility; index generation is handled in `tgnn_prepare`.
    """
    root_path = resolve_root(root)
    names = [n.strip() for n in only] if only else ["wikipedia", "reddit", "simulate_v1", "simulate_v2", "multihost"]
    for name in names:
        ensure_real_dataset(
            name,
            root=root_path,
            force=force,
            do_process=do_process,
            raw_dir=data_dir,
            processed_dir=proc_dir,
        )


# -----------------------------------------------------------------------------
# CLI
# -----------------------------------------------------------------------------

def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:
    p = argparse.ArgumentParser(description="TGNN real dataset setup (download + process)")
    p.add_argument("--root", type=str, default=None, help="Repo root (defaults to CWD or env var ROOT/REPO_ROOT/PROJECT_ROOT)")
    p.add_argument("--only", type=str, default=None, help="Comma list: wikipedia,reddit,simulate_v1,simulate_v2,multihost")
    p.add_argument("--force", action="store_true", help="Re-download even if file exists")
    p.add_argument("--no-process", dest="do_process", action="store_false", help="Skip processing (only download)")
    return p.parse_args(argv)


def main(argv: Optional[Sequence[str]] = None) -> None:
    args = parse_args(argv)
    only = [s.strip() for s in args.only.split(",")] if args.only else None
    setup_tgnn_data(
        root=args.root,
        only=only,
        force=args.force,
        do_process=args.do_process,
    )


if __name__ == "__main__":
    main()
